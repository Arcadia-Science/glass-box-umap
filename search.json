[
  {
    "objectID": "index_v1.0.html",
    "href": "index_v1.0.html",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "",
    "text": "UMAP is a ubiquitous tool for low-dimensional visualization of high-dimensional datasets. UMAP learns a low-dimensional mapping from the nearest-neighbor graph structure of a dataset, often producing visually distinct clusters of data that align with known labels (e.g., cell types in a gene expression dataset). While the learned relationship between the input features and the embedding positions can be useful, the nonlinear UMAP embedding function also makes it difficult to directly interpret the mapping in terms of the input features.\nHere, we show how to enable interpretation of the nonlinear mapping through a modification of the parametric UMAP approach, which learns the embedding with a deep network that is locally linear (but still globally nonlinear) with respect to the input features. This allows for the computation of a set of exact feature contributions as linear weights that determine the embedding of each data point. By computing the exact feature contribution for each point in a dataset, we directly quantify which features are most responsible for forming each cluster in the embedding space. We explore the feature contributions for a gene expression dataset from this “glass-box” augmentation of UMAP and compare them with features found by differential expression."
  },
  {
    "objectID": "index_v1.0.html#purpose",
    "href": "index_v1.0.html#purpose",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "",
    "text": "UMAP is a ubiquitous tool for low-dimensional visualization of high-dimensional datasets. UMAP learns a low-dimensional mapping from the nearest-neighbor graph structure of a dataset, often producing visually distinct clusters of data that align with known labels (e.g., cell types in a gene expression dataset). While the learned relationship between the input features and the embedding positions can be useful, the nonlinear UMAP embedding function also makes it difficult to directly interpret the mapping in terms of the input features.\nHere, we show how to enable interpretation of the nonlinear mapping through a modification of the parametric UMAP approach, which learns the embedding with a deep network that is locally linear (but still globally nonlinear) with respect to the input features. This allows for the computation of a set of exact feature contributions as linear weights that determine the embedding of each data point. By computing the exact feature contribution for each point in a dataset, we directly quantify which features are most responsible for forming each cluster in the embedding space. We explore the feature contributions for a gene expression dataset from this “glass-box” augmentation of UMAP and compare them with features found by differential expression."
  },
  {
    "objectID": "index_v1.0.html#introduction",
    "href": "index_v1.0.html#introduction",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Introduction",
    "text": "Introduction\nUMAP (Uniform Manifold Approximation and Projection) is a powerful tool for nonlinear dimensionality reduction (McInnes et al., 2018). Despite some critical appraisals focused on the use of relative distances over the nonlinear embedding space to generate hypotheses as well as the black-box nature of the nonlinear mapping (Chari and Pachter, 2023; Ng et al., 2023), UMAP remains popular in many fields. Here, we present an augmentation to conventional UMAP analysis that generates exact feature attributions for each point in the dataset.\nPrincipal components analysis (PCA) is another popular method for dimensionality reduction, which finds an alternative linear representation of a dataset by determining orthogonal directions of maximal variance. Since the principal components are the linear weights on input features, this approach is directly interpretable in feature space.\nThe recent popularity of UMAP comes at the cost of interpretable embeddings due to its nonlinearity. While nonlinear methods are generally thought to be black boxes, there are a range of post-hoc feature attribution methods that provide some measure of interpretability (like differential expression applied for gene expression data using ScanPy (v1.11.4) (Wolf et al., 2018) as well as GradCAM (Selvaraju et al., 2017) for image data). UMAP is popular due to its ability to successfully cluster classes for complex datasets in an unsupervised manner, despite its black-box nature.\nUMAP generates distinct clusters as a black box, while PCA provides (sometimes less distinct) clusters for complex datasets, but also provides exact feature contributions. What if we could have the best of both approaches? A technique for interpreting nonlinear deep networks (Mohan et al., 2019; Wang et al., 2016) provides the key for bringing exact feature interpretability to UMAP."
  },
  {
    "objectID": "index_v1.0.html#method",
    "href": "index_v1.0.html#method",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Method",
    "text": "Method\nUMAP embeds high-dimensional data into a low-dimensional representation by building a nearest-neighbor graph in the original space and directly learning a set of embeddings in the representation space that best preserves local and global components of the nearest-neighbor graph according to a loss function.\nThe extension to a “parametric” form of UMAP (Pascarelli, 2023; Sainburg et al., 2021), where a deep network learns a function to generate a low-dimensional mapping, is a valuable generalization of the embedding approach, allowing new data points to be quickly embedded using the same mapping function. The deep network is trained using the same loss as the non-parametric model, so the network function captures the same relationships as the original non-parametric implementation.\nThe deep network approach for parametric UMAP is conventionally considered to be opaque to feature attribution. However, by leveraging a growing body of work in this area, we can implement a deep network with a specific architecture that enables us to measure the exact contributions of each input feature.\nIn Wang et al. (2016), Mohan et al. (2019) and Elhage et al. (2021), it is demonstrated that deep networks with zero-bias linear layers and specific types of activation functions possess exactly equivalent linear mappings. Even though these networks are globally nonlinear, which is why deep networks can learn such complex mappings, they are also locally linear or “point-wise” linear for a particular input (Golden, 2025). These networks fall into the category of homogeneous functions of order 1, which, based on Euler’s theorem, means a function has an equivalent representation with the Jacobian \\(J\\) which varies as a function of \\(x\\):\n\\[\ny(x) = J(x) \\cdot x\n\\tag{1}\\]\nThis mapping is linear and exact, although the Jacobian must be numerically computed for each input. Linear representations offer a straightforward approach to understanding what the network is computing. They are more interpretable than locally nonlinear networks (which include any deep network with nonzero bias terms in its linear layers).\nIt is straightforward to compute these linear feature contributions for every point in a dataset with a GPU. These types of deep networks for genomics data are locally linear, and from an interpretability perspective, they are effectively globally linear. We can easily perform an exhaustive local analysis, where we compute the Jacobian (via autograd) for every point in a dataset. With globally linear systems, there is only one set of feature weights to analyze. However, with locally linear systems, there are as many Jacobians (feature weights) as data points, which adds an additional step to the analysis.\nThe exactness of this Jacobian approach is the centerpiece of its appeal. This local analysis is similar to SHAP (Lundberg and Lee, 2017), LIME (Ribeiro et al., 2016), and GradCAM (Selvaraju et al., 2017); however, these methods are approximations that may be incorrect for the actual nonlinear function. The Jacobian of a zero-bias ReLU deep network weights each feature linearly, quantifying how the globally nonlinear network uses those features to generate its output.\nHere, we apply these deep networks with linear equivalents to UMAP. In many papers, UMAP is presented as an interesting visual representation of data, but it is not frequently used beyond that. Conventionally, differential expression is applied to various clusters to identify genes that are differentially expressed on average (which is distinct from the features used by UMAP). In contrast, with fully interpretable glass-box networks, we can compute the exact contribution of each gene to the position of every single cell shown in the UMAP embedding space. Now, the exact gene feature contributions can be directly extracted from the nonlinear UMAP function, rather than from differential expression, which only acts as a proxy for what UMAP has learned. Additionally, the Jacobian approach works equally as well for image or protein embedding representations, where tools like differential expression are not available.\nBeyond the feature attributions for each point, these can be aggregated over categories (like Leiden cluster or cell type) to generate hypotheses about the gene features connected to phenotypes represented in the dataset (York and Mets, 2025). This can be done in a straightforward manner by computing the feature attributions for all points of a given cell type (or Leiden cluster) and measuring summary statistics, like the mean or the singular value decomposition of the feature contributions.\nThe use of glass-box deep networks for UMAP, therefore, provides clarity into what the UMAP embedding function has actually learned."
  },
  {
    "objectID": "index_v1.0.html#loading-the-data-and-tools",
    "href": "index_v1.0.html#loading-the-data-and-tools",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Loading the data and tools",
    "text": "Loading the data and tools\nFor an example dataset, we will use the human bone marrow gene expression data of Luecken et al. (2021), which is the example dataset now included in ScanPy (v1.11.4).\n\n\nConfigure training parameters\nimport os\nimport anndata as ad\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import font_manager as fm, pyplot as plt\n\n# Config\nTRAIN = False \n\nN_FITS = 16\nN_FITS_TO_LOAD = 1\nN_PCS = 50\nEPOCHS = 64\nRANDOM_STATE = 42\nGROUPBY_KEY = 'cell_type'\nMODEL_PATH_PATTERN = \"models/umap_{i}.pth\"\nSUMMARY_BASENAME = \"saved_outputs/bmmc_features_rev\"\nBATCH_KEY = \"Samplename\"\n\nsummary_stats_file = f\"{SUMMARY_BASENAME}_stats.csv\"\nsummary_plot_file = f\"{SUMMARY_BASENAME}_plot_data.npz\"\nsummary_interactive_file = f\"{SUMMARY_BASENAME}_interactive.csv\"\n\n\n\n\nData and preprocessing methods\nimport os\nimport subprocess\nimport scanpy as sc\nimport anndata as ad\nimport pandas as pd\n\ndef download_bone_marrow_data(\n    url=\"ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE194nnn/GSE194122/suppl/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad.gz\",\n    filename=\"GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad.gz\"\n) -&gt; ad.AnnData:\n    \"\"\"\n    Downloads, unzips, and loads the bone marrow dataset.\n    \"\"\"\n    unzipped_filename = filename.replace(\".gz\", \"\")\n    if not os.path.isfile(unzipped_filename):\n        if not os.path.isfile(filename):\n            subprocess.run([\"wget\", url, \"--no-verbose\"])\n        subprocess.run([\"gunzip\", filename])\n    \n    return sc.read_h5ad(unzipped_filename)\n\ndef preprocess_adata(\n    adata: ad.AnnData,\n    min_genes: int = 100,\n    min_cells: int = 3,\n    n_top_genes: int = 2000,\n    n_pcs: int = 50,\n    batch_key: str = \"Samplename\",\n    run_scrublet: bool = False\n) -&gt; ad.AnnData:\n    \"\"\"\n    Runs the full scRNA-seq preprocessing pipeline on an AnnData object.\n\n    Args:\n        adata (ad.AnnData): The raw AnnData object.\n        min_genes (int): Min genes for cell filtering.\n        min_cells (int): Min cells for gene filtering.\n        n_top_genes (int): Number of highly variable genes to select.\n        n_pcs (int): Number of principal components to compute.\n        batch_key (str): The key in .obs for batch correction (if any).\n        run_scrublet (bool): Whether to run doublet detection.\n\n    Returns:\n        ad.AnnData: The processed AnnData object.\n    \"\"\"\n    print(\"--- Starting Preprocessing ---\")\n    \n    # 1. Initial setup and QC gene flagging\n    adata.obs_names_make_unique()\n    adata.var_names_make_unique()\n    adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n    adata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n    adata.var[\"hb\"] = adata.var_names.str.contains(\"^HB[^(P)]\")\n    \n    # 2. Calculate QC\n    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, log1p=True)\n    \n    # 3. Remove QC genes\n    genes_to_remove = adata.var[\"mt\"] | adata.var[\"ribo\"] \n    adata._inplace_subset_var(~genes_to_remove)\n    \n    # 4. Filter cells, genes, and detect doublets\n    sc.pp.filter_cells(adata, min_genes=min_genes)\n    sc.pp.filter_genes(adata, min_cells=min_cells)\n    if run_scrublet: \n        sc.pp.scrublet(adata, batch_key=batch_key)\n    \n    # 5. Normalize and find HVGs\n    adata.layers[\"counts\"] = adata.X.copy()\n    sc.pp.normalize_total(adata)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, batch_key=batch_key)\n    \n    # 6. Run PCA\n    sc.tl.pca(adata, n_comps=n_pcs, use_highly_variable=True)\n    \n    return adata\n\n\nThe preprocessing pipeline follows the standard procedure for the dataset in the ScanPy (v1.11.4) clustering tutorial. We take the extra step of filtering out the less common cell types to simplify visualizations and keep only the top 12.\n\n\nLoad and preprocess data\ndef prepare_data(groupby_key: str = 'cell_type', n_pcs: int = 50, batch_key: str = \"Samplename\") -&gt; ad.AnnData:\n    \"\"\"\n    Loads, concatenates, and preprocesses the scRNA-seq data.\n    \"\"\"\n    adata_raw = download_bone_marrow_data()\n    adata_raw.var_names_make_unique()\n\n    # Slice and concatenate data\n    if groupby_key == 'cell_type':\n        adata_subset1 = adata_raw[adata_raw.obs['Samplename'] == 'site1_donor1_cite', :].copy()\n        adata_subset3 = adata_raw[adata_raw.obs['Samplename'] == 'site1_donor3_cite', :].copy()\n        adata_filtered = ad.concat([adata_subset1, adata_subset3], label=\"donors\")\n    else:\n        adata_filtered = adata_raw\n        \n    adata_filtered.obs_names_make_unique()\n\n    # Run the preprocessing pipeline on ALL concatenated cells first\n    adata_processed = preprocess_adata(\n        adata_filtered,  # &lt;-- Use the unfiltered data\n        n_top_genes=2000,\n        n_pcs=n_pcs,\n        batch_key=batch_key\n    )\n\n    # Now, filter to the top cell types for your analysis\n\n    if groupby_key == 'cell_type':\n        top_cell_types = adata_processed.obs[groupby_key].value_counts().nlargest(12).index\n        # This subset now contains the .obsm['X_pca'] that was calculated on ALL cells\n        adata_final = adata_processed[adata_processed.obs[groupby_key].isin(top_cell_types)].copy()\n    if groupby_key.lower() == 'cd4+_t_cell_type':  \n        adata_subset1 = adata_processed[adata_processed.obs['cell_type'] == 'CD4+ T activated', :].copy()\n        adata_subset3 = adata_processed[adata_processed.obs['cell_type'] == 'CD4+ T naive', :].copy()\n\n        adata_t_cells = ad.concat([adata_subset1,adata_subset3], label=\"T_cell_type\")\n\n        adata_final = scp.preprocess_adata(\n            adata_t_cells,\n            n_top_genes=1000, # You can use fewer HVGs for a subset\n            n_pcs=50,         # You need fewer PCs for a subset\n            batch_key=\"Samplename\"\n        )\n\n    return adata_final\n\nadata_final = prepare_data(\n        groupby_key=GROUPBY_KEY, \n        n_pcs=N_PCS, \n        batch_key=BATCH_KEY\n    )\n\n\nWe will also load a set of tools including the “UMAP PyTorch” toolbox in addition to ScanPy (v1.11.4), and define a custom PyTorch (v2.9.0) network to learn an embedding.\nWe perform several independent UMAP fits to the data, starting from different random initializations, to generate error bars for the feature contributions.\n\n\nGlassBoxUMAP class\nimport os\nimport sys\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\n# --- Import PUMAP submodule ---\n# (Assuming 'external/umap_pytorch' is in the working directory)\nSUBMODULE_RELATIVE_PATH = 'external/umap_pytorch' \nproject_root = os.getcwd()\nsubmodule_root = os.path.join(project_root, SUBMODULE_RELATIVE_PATH)\nif submodule_root not in sys.path:\n    sys.path.insert(0, submodule_root) \ntry:\n    from umap_pytorch.main import PUMAP\nexcept ImportError:\n    print(f\"Error: Could not import PUMAP from {submodule_root}.\")\n    print(\"Please ensure the submodule exists and is initialized.\")\n    sys.exit(1)\n# ------------------------------\n\nclass GlassBoxUMAP:\n    \"\"\"\n    Encapsulates the parametric UMAP model fitting and feature attribution.\n    \n    This class follows a scikit-learn style API:\n    1. Initialize with hyperparameters.\n    2. Fit with pre-processed data (e.g., PCA).\n    3. Compute attributions using PCA data, gene expression, and PCA components.\n    \"\"\"\n    def __init__(self,\n                 # PUMAP params\n                 n_components: int = 2,\n                 n_neighbors: int = 15, \n                 min_dist: float = 0.3, \n                 repulsion_strength: float = 3.0,\n                 # Training params\n                 n_fits: int = 1, \n                 epochs: int = 64, \n                 lr: float = 1e-4, \n                 batch_size: int = 2048, \n                 random_state: int = 12,\n                 # Network params\n                 input_size: int = 50,\n                 hidden_size: int = 1024 + 128\n                 ):\n        \"\"\"Initializes the model with all hyperparameters.\"\"\"\n        self.n_components = n_components\n        self.n_neighbors, self.min_dist, self.repulsion_strength = n_neighbors, min_dist, repulsion_strength\n        self.n_fits, self.epochs, self.lr, self.batch_size, self.random_state = n_fits, epochs, lr, batch_size, random_state\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.models_ = []\n        self.embeddings_ = []\n        self.jacobians_ = []\n        self.feature_contributions_ = []\n        self.device_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    def fit(self, X: np.ndarray, \n            load_models: bool = False, \n            load_n_fits: int = 1,\n            save_models: bool = True,\n            model_path_pattern: str = \"models/umap_{i}.pth\"):\n        \"\"\"\n        Fits the Parametric UMAP model to the input data X.\n\n        Args:\n            X (np.ndarray): The input data (e.g., PCA embeddings), \n                            shape (n_samples, n_features).\n            load_models (bool): If True, skips training and loads pre-trained\n                                models from `model_path_pattern`.\n            save_models (bool): If True, saves the trained model weights to\n                                `model_path_pattern` after fitting.\n            model_path_pattern (str): A string pattern for the model file paths.\n        \"\"\"\n        self.train_data_ = torch.tensor(X, dtype=torch.float32)\n        \n        self.models_ = []\n        self.embeddings_ = []\n\n        for i in range(self.n_fits):\n            set_global_seeds(2*self.random_state + i)\n            network = deepReLUNet(\n                input_size=self.input_size, \n                hidden_size=self.hidden_size,\n                output_size=self.n_components\n            )\n            \n            pumap_model = PUMAP(\n                encoder=network, n_neighbors=self.n_neighbors, \n                min_dist=self.min_dist, random_state=self.random_state + i,\n                lr=self.lr, epochs=0 if load_models else self.epochs, # Only 1 epoch if loading\n                batch_size=self.batch_size, num_workers=8, num_gpus=1\n            )\n\n            # Train or load\n            model_file = model_path_pattern.format(i=i)\n            \n            if load_models:\n                if i &lt; load_n_fits:                                \n                    try:\n                        # print(pumap_model.trainer)\n                        # pumap_model.trainer.load_from_checkpoint('/home/ubuntu/james/glass-box-umap/lightning_logs/version_110/checkpoints/epoch=63-step=320.ckpt')\n                        pumap_model.device = self.device_\n                        pumap_model.encoder.to(self.device_)\n                        # We must \"fit\" with 1 epoch to initialize the graph\n                        pumap_model.fit(self.train_data_) \n                        pumap_model.encoder.load_state_dict(\n                            torch.load(model_file, map_location=self.device_)\n                        )\n                        pumap_model.encoder.eval() \n\n                        self.models_.append(pumap_model)\n                        embedding = pumap_model.transform(self.train_data_) \n                        self.embeddings_.append(embedding)\n\n                    except FileNotFoundError:\n                        print(f\"Error: Model file not found at {model_file}\")\n                        raise\n                    except Exception as e:\n                        print(f\"Error loading state dict for model {i}: {e}\")\n                        raise\n                    \n            else:\n                pumap_model.fit(self.train_data_)\n\n                if save_models:\n                    os.makedirs(os.path.dirname(model_file), exist_ok=True)\n                    torch.save(pumap_model.encoder.state_dict(), model_file)\n            \n                self.models_.append(pumap_model)\n                embedding = pumap_model.transform(self.train_data_) \n                self.embeddings_.append(embedding)\n            \n        return self\n\n    def transform(self, X: np.ndarray, fit_index: int = 0) -&gt; np.ndarray:\n        \"\"\"\n        Transforms new data X into the embedding space using a trained model.\n\n        Args:\n            X (np.ndarray): New data to transform.\n            fit_index (int): Index of the model to use for transformation.\n\n        Returns:\n            np.ndarray: The UMAP embedding.\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"The model must be fitted before transforming.\")\n        if fit_index &gt;= len(self.models_):\n            raise IndexError(\"fit_index is out of bounds.\")\n        \n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        return self.models_[fit_index].transform(X_tensor)\n\n    def fit_transform(self, X: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"\n        Fits the model to X and returns the embedding of X.\n        \n        Args:\n            X (np.ndarray): The input data (e.g., PCA embeddings).\n            **kwargs: Additional arguments passed to self.fit().\n\n        Returns:\n            np.ndarray: The UMAP embedding for the first fit (fit_index=0).\n        \"\"\"\n        self.fit(X, **kwargs)\n        return self.embeddings_[0]\n\n    def compute_attributions(self, \n                             X_centered_gene_expression: np.ndarray, \n                             pca_components: np.ndarray, \n                             jacobian_batch_size: int = 40):\n        \"\"\"\n        Computes the Jacobian and projects it to the original gene space.\n\n        Args:\n            X_centered_gene_expression (np.ndarray): Mean-centered gene expression\n                data, shape (n_samples, n_genes).\n            pca_components (np.ndarray): The PCA loading matrix (e.g., adata.varm[\"PCs\"]),\n                shape (n_genes, n_pcs).\n            jacobian_batch_size (int): Batch size for Jacobian calculation.\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"The model must be fitted before computing contributions.\")\n        if self.train_data_ is None:\n             raise RuntimeError(\"self.train_data_ not set. Please call fit() first.\")\n        \n        self.feature_contributions_ = []\n        self.jacobians_ = []\n\n        for i, model in enumerate(self.models_):\n            encoder = model.encoder\n            encoder.eval()\n            \n            # 1. Compute Jacobian in batches (in PCA space)\n            num_samples = self.train_data_.shape[0]\n            jacobians_pca_list = []\n            for j in range(0, num_samples, jacobian_batch_size):\n                data_batch = self.train_data_[j:j + jacobian_batch_size, :]\n                \n                jac_batch = torch.autograd.functional.jacobian(\n                    encoder, data_batch, vectorize=True, strategy=\"reverse-mode\"\n                )\n                # Un-fuse the vectorized diagonal output\n                jac_batch_unfused = torch.einsum('bibj-&gt;bij', jac_batch)\n                jacobians_pca_list.append(jac_batch_unfused.detach().cpu())\n\n            jacobians_pca_tensor = torch.cat(jacobians_pca_list, dim=0)\n\n            # 2. Project Jacobian from PCA space back to gene space\n            # J_gene[i, emb, gene] = J_pca[i, emb, pc] * PCs[gene, pc]\n            gene_space_jacobian = torch.einsum(\n                'bij,kj-&gt;bik', \n                jacobians_pca_tensor, \n                torch.tensor(pca_components, dtype=torch.float32)\n            )\n\n            # 3. Weight by each cell's mean-centered gene expression\n            feature_contributions = gene_space_jacobian.numpy() * X_centered_gene_expression[:, np.newaxis, :]\n            \n            # For memory efficiency\n            feature_contributions = feature_contributions.astype('float16')\n            \n            self.feature_contributions_.append(feature_contributions)\n            self.jacobians_.append(jacobians_pca_tensor)\n\n        return self\n\n    def get_feature_importance(self, \n                               adata: 'ad.AnnData', \n                               groupby: str,\n                               gene_names: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"\n        Aggregates feature contributions by a specified group.\n\n        Args:\n            adata (ad.AnnData): The AnnData object, needed for .obs groupings.\n            groupby (str): The column in adata.obs to group by (e.g., 'cell_type').\n            gene_names (np.ndarray): An array of gene names.\n\n        Returns:\n            pd.DataFrame: A DataFrame with mean and SEM contributions for\n                          each gene in each group.\n        \"\"\"\n        if not self.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first.\")\n\n        all_run_jacobians = np.array(self.feature_contributions_)\n        n_runs = all_run_jacobians.shape[0]\n        all_groups = adata.obs[groupby].cat.categories\n        \n        summary_dfs = []\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            if np.sum(is_group_mask) == 0:\n                continue # Skip if group has no cells\n\n            # Shape: (n_runs, n_cells_in_group, n_dims, n_genes)\n            jacobians_for_group = all_run_jacobians[:, is_group_mask, :, :]\n            \n            # Calculate magnitudes (L2 norm across UMAP dims)\n            # Shape: (n_runs, n_cells_in_group, n_genes)\n            magnitudes = np.linalg.norm(jacobians_for_group, axis=2, ord=2)\n            \n            run_mean_contributions = []\n            for run_idx in range(n_runs):\n                run_mags = magnitudes[run_idx, :, :] # (n_cells, n_genes)\n                \n                # Normalize each cell by its own total contribution\n                cell_sums = np.sum(run_mags, axis=1, keepdims=True)\n                normalized_mags = run_mags / (cell_sums + 1e-9)\n\n                # Get the mean contribution for each gene across cells *for this run*\n                run_mean_contributions.append(np.mean(normalized_mags, axis=0))\n            \n            # Aggregate across all runs\n            # Shape: (n_runs, n_genes)\n            run_means_array = np.array(run_mean_contributions) \n            \n            # Final stats across runs\n            mean_contributions = np.mean(run_means_array, axis=0)\n            sem_contributions = np.std(run_means_array, axis=0) / np.sqrt(n_runs)\n\n            df = pd.DataFrame({\n                'gene': gene_names, \n                'mean_contribution': mean_contributions,\n                'sem_contribution': sem_contributions, \n                groupby: group\n            })\n            summary_dfs.append(df)\n            \n        return pd.concat(summary_dfs, ignore_index=True)\n\n\n    def save_analysis_summary(self, \n                              adata: 'ad.AnnData', \n                              groupby: str,\n                              basename: str = \"analysis_summary\"):\n        \"\"\"\n        Saves all necessary data for offline plotting and analysis.\n        \n        Args:\n            adata (ad.AnnData): The AnnData object, needed for .obs groupings\n                                and gene names.\n            groupby (str): The column in adata.obs to group by (e.g., 'cell_type').\n            basename (str): The prefix for the three output files.\n        \"\"\"\n        if (not self.feature_contributions_ or \n            not self.embeddings_ or \n            not self.jacobians_ or \n            self.train_data_ is None):\n            raise RuntimeError(\"Must run fit() and compute_attributions() first.\")\n\n        # 1. Save population-level statistics\n        stats_df = self.get_feature_importance(adata, groupby, adata.var_names.values)\n        stats_filename = f\"{basename}_stats.csv\"\n        stats_df.to_csv(stats_filename, index=False)\n\n        # 2. Save plot data (NPZ)\n        mean_vector_dict = {}\n        all_groups = adata.obs[groupby].cat.categories\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            mean_vector_dict[group] = np.mean(self.feature_contributions_[0][is_group_mask], axis=0)\n            \n        jacobxall_first_run = self.feature_contributions_[0]\n        jacobian_magnitude = np.linalg.norm(jacobxall_first_run, axis=1)\n\n        jacobian_0 = self.jacobians_[0]\n        pca_data_0 = self.train_data_.squeeze().detach().cpu().numpy()\n        reconstruction_0 = np.einsum('ijk,ik-&gt;ij', jacobian_0.numpy(), pca_data_0)\n\n        plot_data_filename = f\"{basename}_plot_data.npz\"\n        np.savez_compressed(\n            plot_data_filename,\n            embedding=self.embeddings_[0],\n            group_labels=adata.obs[groupby].values,\n            group_by_key=groupby, # Store the key name\n            mean_jacobian_vectors=mean_vector_dict,\n            jacobian_magnitude=jacobian_magnitude,\n            gene_names=adata.var_names.values,\n            jacobian_reconstruction=reconstruction_0  \n        )\n\n        # 3. Save interactive plot data\n        interactive_df = self._prepare_plotly_df(\n            adata, groupby=groupby, fit_index=0, top_n_genes=8\n        )\n        interactive_filename = f\"{basename}_interactive.csv\"\n        interactive_df.to_csv(interactive_filename, index=False)\n\n    def _prepare_plotly_df(self, \n                           adata: 'ad.AnnData', \n                           groupby: str, \n                           fit_index: int = 0, \n                           top_n_genes: int = 8) -&gt; pd.DataFrame:\n        \"\"\"(Private) Prepares a DataFrame for interactive plotting.\"\"\"\n        embedding = self.embeddings_[fit_index]\n        jacobxall = self.feature_contributions_[fit_index]\n        \n        df = pd.DataFrame(embedding, columns=['UMAP 0', 'UMAP 1'])\n        df[groupby] = adata.obs[groupby].values\n\n        # Calculate squared distance and find top contributing genes\n        gene_dist_sq = jacobxall[:, 0, :]**2 + jacobxall[:, 1, :]**2\n        genes = adata.var.index.values\n        \n        top_gene_indices = np.argsort(gene_dist_sq, axis=1)[:, ::-1][:, :top_n_genes]\n        \n        for i in range(top_n_genes):\n            df[f'gene_{i}'] = genes[top_gene_indices[:, i]]\n            \n        return df\n\n\n\n\nPyTorch (v2.9.0) MLP for UMAP\nfrom torch import nn\nclass LayerNormDetached(nn.Module):\n    '''\n    A LayerNorm implementation where the variance calculation is detached from the\n    computation graph during evaluation, potentially stabilizing training.\n    '''\n    def __init__(self, emb_dim: int):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        '''Forward pass for LayerNormDetached.'''\n        mean = x.mean(dim=-1, keepdim=True)\n        # Detach variance calculation during evaluation\n        if not self.training:\n            var = x.clone().detach().var(dim=-1, keepdim=True, unbiased=False)\n        else:\n            var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n        norm_x = (x - mean) / torch.sqrt(var + 1e-12) # Added epsilon for stability\n        return self.scale * norm_x\n\nclass deepReLUNet(nn.Module):\n    \"\"\"\n    A deep neural network using PReLU activation and LayerNormDetached.\n    \"\"\"\n    def __init__(self, input_size: int = 50, hidden_size: int = 256, output_size: int = 2):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(),\n            nn.Linear(hidden_size, output_size, bias=False)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the PReLU network.\"\"\"\n        return self.model(x)\n\nimport random\nimport random\nimport numpy as np\n# You may also need: import pytorch_lightning as pl\n\ndef set_global_seeds(seed: int):\n    \"\"\"Sets global seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    # Optional: For the Pytorch Lightning trainer\n    # pl.seed_everything(seed) \n    \n    # You might also want deterministic algorithms\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n\n\nFit models for UMAP\n# === 2. Model Fitting ===\nprint(\"\\n=== 2. Model Fitting ===\")\nreducer = GlassBoxUMAP(\n    n_fits=N_FITS,\n    epochs=EPOCHS if TRAIN else 0, \n    random_state=RANDOM_STATE,\n    input_size=N_PCS\n)\n\nreducer.fit(\n    adata_final.obsm['X_pca'],\n    load_models=not TRAIN,\n    load_n_fits=N_FITS_TO_LOAD,\n    save_models=TRAIN,\n    model_path_pattern=MODEL_PATH_PATTERN\n)"
  },
  {
    "objectID": "index_v1.0.html#umap-of-gene-expression-data",
    "href": "index_v1.0.html#umap-of-gene-expression-data",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "UMAP of gene expression data",
    "text": "UMAP of gene expression data\nHere we show the embeddings from the convetional, non-parametric UMAP from ScanPy (v1.11.4) as well as the PyTorch (v2.9.0) version of parametric UMAP. For visualizations we use the Arcadia Pycolor toolbox (“Arcadia-pycolor,” 2025) (v0.6.5).\n\n\nPlotting methods\n# pumap_plotting\n\nimport scanpy as sc\nimport anndata as ad\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom adjustText import adjust_text\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport arcadia_pycolor as apc\n\nimport logging\nlogging.getLogger(\"scanpy.runtime\").setLevel(logging.ERROR) \n\ndef setup_plotting_themes():\n    \"\"\"Sets up the plotting themes for matplotlib and plotly.\"\"\"\n    apc.mpl.setup()\n    apc.plotly.setup()\n\ndef plot_scanpy_umap(adata: ad.AnnData, \n                     groupby: str = 'cell_type', \n                     n_neighbors: int = 15, \n                     random_state: int = 13):\n    \"\"\"\n    Computes and plots the standard non-parametric UMAP using Scanpy.\n    \n    Args:\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to color by.\n        n_neighbors (int): Number of neighbors for UMAP.\n        random_state (int): Random state for UMAP.\n    \"\"\"\n    \n    if 'neighbors' not in adata.uns:\n        sc.pp.neighbors(adata, n_neighbors=n_neighbors, use_rep='X_pca')\n    \n    # Compute standard UMAP\n    sc.tl.umap(adata, init_pos='random', random_state=random_state)\n\n    # Order categories by frequency\n    category_order = adata.obs[groupby].value_counts().index.tolist()\n    adata.obs[groupby] = adata.obs[groupby].astype('str').astype(\n        pd.CategoricalDtype(categories=category_order, ordered=True)\n    )\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        ax = sc.pl.umap(\n            adata, color=groupby, size=2,\n            palette=list(apc.palettes.primary), show=False\n        )\n        ax.set_xlabel(\"UMAP 0\")\n        ax.set_ylabel(\"UMAP 1\")\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.set_title(\"\")#f\"{groupby} (Standard UMAP)\")\n        plt.show()\n\ndef plot_parametric_umap(reducer: GlassBoxUMAP, \n                         adata: ad.AnnData, \n                         fit_index: int = 0, \n                         groupby: str = 'cell_type'):\n    \"\"\"\n    Plots the UMAP embedding from a specific parametric model fit.\n\n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        fit_index (int): The index of the fitted model to visualize.\n        groupby (str): The .obs column to color by.\n    \"\"\"\n    if not reducer.embeddings_:\n        raise RuntimeError(\"The reducer must be fitted before plotting.\")\n    \n    resolved_index = fit_index if fit_index &gt;= 0 else len(reducer.embeddings_) + fit_index\n\n    # Order categories by frequency\n    category_order = adata.obs[groupby].value_counts().index.tolist()\n    adata.obs[groupby] = adata.obs[groupby].astype('str').astype(\n        pd.CategoricalDtype(categories=category_order, ordered=True)\n    )\n        \n    # Temporarily assign the parametric embedding to the default UMAP slot\n    adata.obsm['X_umap'] = reducer.embeddings_[fit_index]\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        ax = sc.pl.umap(\n            adata, use_raw=False, color=groupby, size=2,\n            palette=list(apc.palettes.primary),\n            show=False\n        )\n        ax.set_xlabel(\"UMAP 0\")\n        ax.set_ylabel(\"UMAP 1\")\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.set_title(\"\")#\n        # ax.set_title(f\"{groupby} (Parametric UMAP Fit {resolved_index})\")\n        plt.show()\n            \ndef validate_jacobian(\n    reducer: 'GlassBoxUMAP', # Use quotes if class is not yet defined\n    fit_index: int = 0, \n    n_samples: int = 100,\n    dtype: torch.dtype = torch.float64\n):\n    \"\"\"\n    Computes and plots the UMAP embedding vs. its reconstruction from the \n    on-the-fly Jacobian, and plots the reconstruction error.\n    \n    This function uses the computation method from your 'plot_error' example.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        fit_index (int): The index of the fitted model to use.\n        n_samples (int): The number of samples to use for the validation \n                         (from the start of the training set).\n        dtype (torch.dtype): The dtype (e.g., torch.float64) for the computation.\n    \"\"\"\n    \n    if (not reducer.models_ or not reducer.train_data_ is not None):\n        raise RuntimeError(\n            \"Must run fit() first to have models and training data available.\"\n        )\n    if fit_index &gt;= len(reducer.models_):\n        raise IndexError(\"fit_index is out of bounds for reducer.models_.\")\n\n    device = reducer.device_\n    if \"cuda\" not in device:\n        n_samples = 8\n    \n    encoder_casted = reducer.models_[fit_index].encoder.to(device=device, dtype=dtype)\n    encoder_casted.eval()\n\n    if n_samples &gt; reducer.train_data_.shape[0]:\n        n_samples = reducer.train_data_.shape[0]\n        \n    pca_data = reducer.train_data_[:n_samples]\n    data_batch_casted = pca_data.to(device=device, dtype=dtype)\n    \n    jac_batch = torch.autograd.functional.jacobian(\n        encoder_casted, data_batch_casted, vectorize=True, strategy=\"reverse-mode\"\n    )\n    \n    reconstruction = torch.einsum(\n        'bibj,bj-&gt;bi', \n        jac_batch, \n        data_batch_casted\n    )\n    \n    embedding = encoder_casted(data_batch_casted)\n    \n    err = reconstruction - embedding\n    \n    embedding_np = embedding.detach().cpu().numpy()\n    reconstruction_np = reconstruction.detach().cpu().numpy()\n    err_np = err.detach().cpu().numpy()\n\n    try:\n        plot_context = mpl.rc_context({\n            \"figure.facecolor\": apc.parchment, \n            \"axes.facecolor\": apc.parchment\n        })\n    except Exception:\n        print(\"Warning: 'apc.parchment' not found. Using default plot style.\")\n        plot_context = mpl.rc_context({}) \n    with plot_context:\n        \n        plt.figure()\n        \n        plt.scatter(embedding_np[:, :].flatten(), reconstruction_np[:, :].flatten(), alpha=0.5, label=\"Reconstruction (flattened)\")\n        \n        global_min = min(embedding_np.min(), reconstruction_np.min())\n        global_max = max(embedding_np.max(), reconstruction_np.max())\n        \n        # Add a small buffer\n        min_val = global_min - 1\n        max_val = global_max + 1\n        plt.plot([min_val, max_val], [min_val, max_val], 'r', linewidth=2, label=\"Identity (exact)\")\n\n        plt.xlabel('UMAP Embedding')#, labelpad=33.8)\n        plt.ylabel('Jacobian Reconstruction')\n        plt.legend()            \n        # plt.axis('square')\n        plt.xlim(min_val, max_val)\n        plt.ylim(min_val, max_val)\n        ax1=plt.gca()\n        # ax1.set_aspect('equal', adjustable='box')\n        ax1.set_box_aspect(1)\n        plt.show()\n\n        plt.figure()\n        plt.hist(err_np.flatten(), bins=40)\n        # current_ax = plt.gca()\n        # current_ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=5, prune='both'))\n        plt.xlabel('Reconstruction Error')# (Reconstruction - Embedding)', labelpad=20)\n        plt.ylabel('Frequency')\n        ax2=plt.gca()\n        # ax2.set_aspect('equal', adjustable='box')\n        ax2.set_box_aspect(1)\n        # plt.title(f'Histogram of Reconstruction Error ({dtype})')\n        plt.show()\n        \ndef plot_interactive(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    color_by: str = 'group', \n    top_n_to_show: int = 16, \n    show_centroids: bool = False, \n    fit_index: int = 0,\n    summary_file: str = \"analysis_summary_interactive.csv\",\n    show_percentage: bool = False\n):\n    \"\"\"\n    Generates an interactive Plotly UMAP embedding.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n        color_by (str): 'group' (to color by `groupby` key) or 'top_gene'.\n        summary_file (str): Path to the .csv file for loading/saving.\n    \"\"\"\n    import os\n    \n    if summary_file and os.path.exists(summary_file):\n        df = pd.read_csv(summary_file)\n\n        if groupby not in df.columns:\n            print(f\"Warning: Column '{groupby}' not found in {summary_file}.\")\n            \n            potential_cols = [col for col in df.columns if \n                              col not in ['UMAP 0', 'UMAP 1'] and \n                              not col.startswith('gene_')]\n            \n            if len(potential_cols) == 1:\n                old_groupby = potential_cols[0]\n                df = df.rename(columns={old_groupby: groupby})\n            else:\n\n                raise KeyError(\n                    f\"Column '{groupby}' not found in {summary_file}. \"\n                    f\"Found potential group columns: {potential_cols}. \"\n                    \"The summary file may be stale. \"\n                    \"Try running with TRAIN=True to regenerate it.\"\n                )\n\n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate data.\")\n        \n        df = reducer._prepare_plotly_df(adata, groupby, fit_index=fit_index)\n        \n        if summary_file:\n            df.to_csv(summary_file, index=False)\n\n    hover_data = ['gene_0', 'gene_1', 'gene_2', groupby]\n    \n    if color_by == 'group':\n        fig = px.scatter(\n            df, x='UMAP 0', y='UMAP 1', color=groupby,\n            # title=f'Bone Marrow Gene Expression: {groupby}',\n            hover_data={k: True for k in hover_data if k != groupby},\n            category_orders={groupby: df[groupby].astype('category').value_counts().index},\n            color_discrete_sequence=(apc.palettes.primary + apc.palettes.secondary)\n        )\n        grouping_col, data_for_centroids = groupby, df\n    \n    elif color_by == 'top_gene':\n        df['gene_0'] = df['gene_0'].astype(str)\n        top_genes = df['gene_0'].value_counts().nlargest(top_n_to_show).index\n        df_filtered = df[df['gene_0'].isin(top_genes)]\n        percent_shown = len(df_filtered) / len(df)\n        # title = f'Bone Marrow: Top Gene Contributors, ({percent_shown:.1%} of cells shown)'\n        fig = px.scatter(\n            df_filtered, x='UMAP 0', y='UMAP 1', color='gene_0',\n            title=\"\", hover_data=hover_data,\n            category_orders={'gene_0': top_genes},\n            color_discrete_sequence=(apc.palettes.secondary + apc.palettes.primary)\n        )\n        grouping_col, data_for_centroids = 'gene_0', df_filtered\n    \n    else:\n        raise ValueError(\"color_by must be 'group' or 'top_gene'\")\n\n    if show_centroids:\n        centroids = data_for_centroids.groupby(grouping_col)[['UMAP 0', 'UMAP 1']].mean()\n        for label, center in centroids.iterrows():\n            fig.add_annotation(\n                x=center['UMAP 0'], y=center['UMAP 1'], text=f\"&lt;b&gt;{label}&lt;/b&gt;\",\n                showarrow=False, font=dict(size=16, color='black'),\n                align='center', bgcolor='rgba(255, 255, 255, 0.5)', borderpad=4\n            )\n            \n    fig.update_traces(marker_size=3)\n    fig.update_layout(\n        autosize=True, \n        yaxis_scaleanchor=\"x\",\n        legend={'itemsizing': 'constant', 'y': 1, 'x': 1.0, 'yanchor': 'top', 'xanchor': 'left'}\n    )\n\n    apc.plotly.style_plot(fig, monospaced_axes=\"all\")\n    fig.show()\n    if color_by == 'top_gene' and show_percentage:\n        print(f'Bone Marrow: Top Gene Contributors, ({percent_shown:.1%} of cells shown)')\n\ndef plot_feature_importance_by_group(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    n_features_bars: int = 12, \n    n_features_vectors: int = 3,\n    fit_index: int = 0, \n    groups_to_plot: list = None,\n    summary_stats_file: str = \"analysis_summary_stats.csv\",\n    summary_plot_file: str = \"analysis_summary_plot_data.npz\",\n    set_axes_equal: bool = False,\n    plot_sum_features: bool = False\n):\n    \"\"\"\n    Analyzes and visualizes feature contributions for each group.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n        groups_to_plot (list): A list of specific group names to plot. \n                               If None, plots the top 12.\n    \"\"\"\n    import os\n    # Add necessary imports that were implicit in the original\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib as mpl\n    from adjustText import adjust_text\n    # Assuming 'apc' is available in the environment (e.g., import anndata_plotting_context as apc)\n    # Assuming 'ad' (anndata) is available\n    \n    can_load_from_file = (\n        summary_stats_file and os.path.exists(summary_stats_file) and\n        summary_plot_file and os.path.exists(summary_plot_file)\n    )\n    \n    if can_load_from_file:\n        summary_df = pd.read_csv(summary_stats_file)\n        with np.load(summary_plot_file, allow_pickle=True) as data:\n            embedding = data['embedding']\n            \n            try:\n                group_labels_array = data['group_labels']\n                loaded_groupby_key = str(data['group_by_key']) if 'group_by_key' in data else 'cell_type' \n            except KeyError:\n                print(\"Warning: 'group_labels' key not found. Trying fallback 'cell_types'...\")\n                try:\n                    group_labels_array = data['cell_types'] \n                    loaded_groupby_key = 'cell_type' \n                except KeyError:\n                    raise KeyError(\n                        \"Could not find 'group_labels' or 'cell_types' in the .npz file. \"\n                        \"The summary file may be stale or corrupt. \"\n                        \"Try running with TRAIN=True to regenerate it.\"\n                    )\n            \n            mean_jacobian_vectors = data['mean_jacobian_vectors'].item()\n            gene_names_original_order = data['gene_names']\n            \n            if loaded_groupby_key != groupby:\n                print(f\"Warning: File was grouped by '{loaded_groupby_key}', \"\n                      f\"but you requested '{groupby}'. Results may be incorrect.\")\n        \n        all_groups = pd.Series(group_labels_array).value_counts().index\n        \n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate data.\")\n        \n        embedding = reducer.embeddings_[fit_index]\n        group_labels_array = adata.obs[groupby].values\n        all_groups = adata.obs[groupby].value_counts().index\n        gene_names_original_order = adata.var_names.values \n        \n        summary_df = reducer.get_feature_importance(adata, groupby, gene_names_original_order)\n\n        mean_jacobian_vectors = {}\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            mean_jacobian_vectors[group] = np.mean(\n                reducer.feature_contributions_[fit_index][is_group_mask], axis=0\n            )\n\n    if groups_to_plot is None:\n        groups_to_plot = all_groups[:12]\n    \n    # This assumes 'apc' is an imported module available in the scope\n    cmap = (apc.palettes.primary + apc.palettes.secondary).to_mpl_cmap()\n    category_colors = [cmap(i / len(all_groups)) for i in range(len(all_groups))]\n    color_map = {name: color for name, color in zip(all_groups, category_colors)}\n    point_colors = np.array([color_map.get(ct, 'gray') for ct in group_labels_array])\n\n    gene_to_original_index = {gene: i for i, gene in enumerate(gene_names_original_order)}\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        for group in groups_to_plot:\n            \n            # --- Plot 1: Scatter plot with vectors ---\n            \n            # Create new figure without subplots\n            fig1 = plt.figure()#figsize=[6, 6])\n            \n            is_group_mask = (group_labels_array == group)\n\n            # Use plt. scatter, xlabel, ylabel, grid, legend\n            plt.scatter(embedding[:, 0], embedding[:, 1], c=point_colors, s=2, alpha=0.1)\n            plt.scatter(embedding[is_group_mask, 0], embedding[is_group_mask, 1], \n                        c=[color_map.get(group, 'gray')], s=14, marker=\"o\", label=group)\n            plt.xlabel(\"UMAP 0\"); plt.ylabel(\"UMAP 1\")\n            plt.grid(False); plt.legend()\n            \n            # Get current axis for commands that require an axis object\n            current_ax = plt.gca()\n            current_ax.spines[['right', 'top']].set_visible(False)\n            if set_axes_equal:\n                current_ax.set_box_aspect(1)\n            \n            group_df = summary_df[summary_df[groupby] == group].sort_values('mean_contribution', ascending=False)\n            \n            if group_df.empty:\n                print(f\"Skipping plot for {group}: no data found in summary.\")\n                plt.close(fig1) # Close the empty figure\n                continue\n\n            top_bar_indices = group_df.index[:n_features_bars]\n            top_vector_indices = group_df.index[:n_features_vectors]\n            \n            vectors_for_group = mean_jacobian_vectors.get(group)\n            if vectors_for_group is None:\n                print(f\"Skipping vectors for {group}: no mean_jacobian_vectors found.\")\n                continue\n            \n            cluster_centroid = np.mean(embedding[is_group_mask], axis=0)\n            \n            top_gene_names = group_df.loc[top_vector_indices, 'gene']\n            top_gene_original_indices = [gene_to_original_index[gene] for gene in top_gene_names if gene in gene_to_original_index]\n\n            if top_gene_original_indices:\n                max_vector_mag = np.max(np.linalg.norm(vectors_for_group[:, top_gene_original_indices], axis=0))\n                scale_factor = (np.linalg.norm(cluster_centroid) / (max_vector_mag + 1e-6)) * 0.8\n            else:\n                scale_factor = 1.0\n\n            texts = []\n            for gene_idx in top_vector_indices:\n                gene_name = group_df.loc[gene_idx, 'gene']\n                gene_original_index = gene_to_original_index.get(gene_name)\n                \n                if gene_original_index is not None:\n                    vec = vectors_for_group[:, gene_original_index] * scale_factor\n                    # Use plt.arrow and plt.text\n                    plt.arrow(0, 0, vec[0], vec[1], width=0.15, color='k', head_width=0.5, zorder=3)\n                    texts.append(plt.text(vec[0], vec[1], gene_name, fontsize=14,\n                                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=apc.parchment, ec=\"none\", alpha=0.8)))\n            \n            if texts:\n                # adjust_text requires an explicit axis\n                adjust_text(texts, ax=current_ax, arrowprops=dict(arrowstyle=\"-\", color='gray', lw=0.5))\n\n            top_genes = group_df.loc[top_bar_indices, 'gene'][::-1]\n            top_means = group_df.loc[top_bar_indices, 'mean_contribution'][::-1]\n            top_sems = group_df.loc[top_bar_indices, 'sem_contribution'][::-1]\n            \n            if plot_sum_features:\n                # Assuming n_features_vectors is at least 20, or you want the top 'n_features_vectors'\n                # Change the index to top 20 features, or use the existing variable\n                top_path_indices = group_df.index[:220]\n\n                # --- Initialize starting point and list for line segments ---\n                current_pos = np.array([0.0, 0.0])\n                path_points = [current_pos.copy()] # Start the path at (0, 0)\n\n                # --- Calculate the sequential path and plot vectors ---\n                for gene_idx in top_path_indices:\n                    gene_name = group_df.loc[gene_idx, 'gene']\n                    gene_original_index = gene_to_original_index.get(gene_name)\n\n                    if gene_original_index is not None:\n                        \n                        vec = vectors_for_group[:, gene_original_index] \n                        \n                        # --- Plot the vector segment ---\n                        # The vector starts at the end of the previous one (current_pos)\n                        # and points to the new position (current_pos + vec)\n                        plt.arrow(current_pos[0], current_pos[1], vec[0], vec[1], \n                                width=0.15, color='r', head_width=0.0, zorder=4, \n                                label='Sequential Path' if len(path_points) == 1 else None)\n                        \n                        # --- Update position and path points ---\n                        current_pos += vec\n                        path_points.append(current_pos.copy())\n                        \n                        # --- Add text label at the end of the vector segment ---\n                        # plt.text(current_pos[0], current_pos[1], gene_name, fontsize=10, color='r',\n                        #         bbox=dict(boxstyle=\"round,pad=0.1\", fc='white', ec=\"none\", alpha=0.6))\n\n                # --- Plot the path as a single line for clarity (optional) ---\n                path_points_array = np.array(path_points)\n                plt.plot(path_points_array[:, 0], path_points_array[:, 1], 'r--', alpha=0.5, zorder=3)\n\n            fig1.tight_layout()\n            plt.show()\n            \n            # --- Plot 2: Bar chart ---\n            \n            # Create a new, separate figure\n            fig2 = plt.figure()#figsize=[6, 6])\n            \n            # Use plt.barh\n            bars = plt.barh(top_genes, top_means, xerr=top_sems, capsize=3, color=color_map.get(group, 'gray'))\n            \n            # Get current axis for bar_label, spines, and box_aspect\n            current_ax = plt.gca()\n            current_ax.bar_label(bars, labels=[f'{g}' for g in top_genes], padding=5)\n            \n            # Use plt.tick_params, xlabel, ylabel\n            plt.tick_params(axis='y', left=False, labelleft=False)\n            plt.xlabel(\"Normalized feature contribution (mean ± SEM)\") \n            plt.ylabel(\"Genes\")\n            \n            current_ax.spines[['right', 'top']].set_visible(False)\n            if set_axes_equal:\n                current_ax.set_box_aspect(1)\n            current_ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=6, prune='both'))\n            fig2.tight_layout()\n            plt.show()\n            \ndef compare_with_differential_expression(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    n_top_genes: int = 2, \n    summary_stats_file: str = \"analysis_summary_stats.csv\",\n    summary_plot_file: str = \"analysis_summary_plot_data.npz\" \n):\n    \"\"\"\n    Compares Jacobian features with differential expression via dot plots.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n    \"\"\"\n    import os\n    from collections import defaultdict\n    layer_to_use = None\n    \n    can_load_from_file = (\n        summary_stats_file and os.path.exists(summary_stats_file) and\n        summary_plot_file and os.path.exists(summary_plot_file)\n    )\n    \n    if can_load_from_file:\n        summary_df = pd.read_csv(summary_stats_file)\n        \n        jacobian_dict = {}\n        for group in adata.obs[groupby].cat.categories:\n            if group in summary_df[groupby].values:\n                top_genes = summary_df[summary_df[groupby] == group].sort_values(\n                    'mean_contribution', ascending=False\n                )['gene'].values[:n_top_genes]\n                jacobian_dict[group] = list(top_genes)\n        \n        with np.load(summary_plot_file, allow_pickle=True) as data:\n            if 'jacobian_magnitude' not in data or 'gene_names' not in data:\n                print(\"Warning: 'jacobian_magnitude' or 'gene_names' not found in summary file.\")\n            else:\n                loaded_magnitude = data['jacobian_magnitude']\n                loaded_genes = data['gene_names'] # Gene names from the NPZ file\n                \n                # Re-align loaded layer with current adata\n                gene_to_npz_index = {gene: i for i, gene in enumerate(loaded_genes)}\n                new_layer = np.zeros(adata.shape, dtype=loaded_magnitude.dtype)\n                \n                genes_found = 0\n                for i, gene in enumerate(adata.var_names):\n                    if gene in gene_to_npz_index:\n                        new_layer[:, i] = loaded_magnitude[:, gene_to_npz_index[gene]]\n                        genes_found += 1\n                \n                adata.layers['jacobian_magnitude'] = new_layer\n                layer_to_use = 'jacobian_magnitude'\n            \n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate stats.\")\n        \n        stats_df = reducer.get_feature_importance(adata, groupby, adata.var_names.values)\n        \n        jacobian_dict = {}\n        for group in adata.obs[groupby].cat.categories:\n            if group in stats_df[groupby].values:\n                top_genes = stats_df[stats_df[groupby] == group].sort_values(\n                    'mean_contribution', ascending=False\n                )['gene'].values[:n_top_genes]\n                jacobian_dict[group] = list(top_genes)\n        \n        jacobxall_first_run = reducer.feature_contributions_[0]\n        adata.layers['jacobian_magnitude'] = np.linalg.norm(jacobxall_first_run, axis=1)\n        layer_to_use = 'jacobian_magnitude'\n\n    sc.tl.rank_genes_groups(adata, groupby=groupby, method=\"wilcoxon\", n_genes=n_top_genes)\n\n    de_dict = {\n        name: list(adata.uns['rank_genes_groups']['names'][name])\n        for name in adata.uns['rank_genes_groups']['names'].dtype.names\n    }\n\n    combined_genes = defaultdict(list)\n    seen_genes = set()\n    for d in (de_dict, jacobian_dict):\n        for key, value in d.items():\n            for gene in value:\n                if gene not in seen_genes:\n                    combined_genes[key].append(gene)\n                    seen_genes.add(gene)\n    \n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        # Plot 1: Differential Expression\n        ax1 = sc.pl.rank_genes_groups_dotplot(\n            adata, var_names=combined_genes, groupby=groupby, \n            standard_scale=\"var\", show=False\n        )\n        # plt.title(\"Differential expression (Combined DE + Jacobian Genes)\")\n        # plt.show()\n\n        # Plot 2: Jacobian Feature Importance\n        if layer_to_use:\n            ax2 = sc.pl.rank_genes_groups_dotplot(\n                adata, var_names=combined_genes, groupby=groupby, \n                layer=layer_to_use, standard_scale=\"var\", show=False\n            )\n            # plt.title(\"Jacobian feature importance\")\n            # plt.show()\n        else:\n            print(\"Warning: Could not plot Jacobian dot plot. 'jacobian_magnitude' data was not found.\")\n        plt.show()\n\ndef set_fonts():\n    \"\"\"(Optional) Setup custom fonts.\"\"\"\n    font_files = fm.findSystemFonts('Suisse Int_l/')\n    if len(font_files)&gt;0:\n        for font_file in font_files:\n            fm.fontManager.addfont(font_file)\n\nset_fonts()\nsetup_plotting_themes()\n\n\nSince the PyTorch (v2.9.0) UMAP implementation is slightly different than the conventional UMAP (v0.5.9.post2), we generate the embeddings using both approach and show them below in Figure 1.\n\nConventional and parametric UMAP plots\nadata_final.obsm['X_parametric_umap_0'] = reducer.embeddings_[0]\n\nplot_scanpy_umap(adata_final, groupby=GROUPBY_KEY)\n\nplot_parametric_umap(reducer, adata_final, fit_index=0, groupby=GROUPBY_KEY)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The conventional ScanPy (v1.11.4) UMAP embedding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The PyTorch (v2.9.0) network UMAP embedding.\n\n\n\n\n\n\n\nFigure 1: The conventional ScanPy (v1.11.4) UMAP and the PyTorch (v2.9.0) network UMAP. It is also possible to directly fit the embedding learned by the conventional UMAP algorithm, but here we show a fit with the PyTorch (v2.9.0) method to demonstrate how they find similar embeddings."
  },
  {
    "objectID": "index_v1.0.html#exact-decomposition-of-features",
    "href": "index_v1.0.html#exact-decomposition-of-features",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Exact decomposition of features",
    "text": "Exact decomposition of features\nThe Jacobians are computed for each input over the independent fits. This takes a bit of time: about two minutes per fit on a GPU with 16 GB VRAM (on the order of the time spent fitting the model).\n\n\nCompute feature contributions with equivalent linear mapping (ELM) via the Jacobian\nif TRAIN:\n    adata_mean_zero = adata_final.to_df().values - adata_final.to_df().mean(axis=0).values\n    \n    reducer.compute_attributions(\n        X_centered_gene_expression=adata_mean_zero,\n        pca_components=adata_final.varm[\"PCs\"]\n    )\n    print(SUMMARY_BASENAME)\n    reducer.save_analysis_summary(adata_final, groupby=GROUPBY_KEY, basename=SUMMARY_BASENAME)\n\n\nWe can validate that the Jacobian reconstructs the embedding network output below in Figure 2.\n\nValidate the Jacobian reconstruction of the embedding values\nvalidate_jacobian(\n    reducer, \n    n_samples=200\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) UMAP embedding position vs. the Jacobian reconstruction.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram of reconstruction error at float64. The max error is about 3e-14, approaching machine precision.\n\n\n\n\n\n\n\nFigure 2: Jacobian reconstruction. To validate that the Jacobian reconstructs the UMAP encoder network output, we plot the embedding values against their Jacobian reconstructions and see that they fall on the identity line as well as the histogram of the reconstruction error."
  },
  {
    "objectID": "index_v1.0.html#pytorch-v2.9.0-umap-with-feature-labels",
    "href": "index_v1.0.html#pytorch-v2.9.0-umap-with-feature-labels",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "PyTorch (v2.9.0) UMAP with Feature Labels",
    "text": "PyTorch (v2.9.0) UMAP with Feature Labels\nWe can visualize the PyTorch (v2.9.0) embedding and add the top gene contributors to the embedding positions as information in the hovertip in Figure 3. The hovertip information provides feature contributions for each point in the dataset.\n\nEmbedding labeled by cell type\n\nUMAP with top features in hovertip\n#plotly UMAP embedding with data tags\n\nplot_interactive(\n    reducer, \n    adata_final,\n    groupby=GROUPBY_KEY,\n    color_by='group', # or 'top_gene'\n    fit_index=0,\n    summary_file=summary_interactive_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 3: The PyTorch (v2.9.0) UMAP embedding colored by cell type with top genes for each cell labeled in the hover tip.\n\n\n\n\n\n\n\n\nEmbedding labeled by top gene contributor\nThe embedding position can also be colored by the top gene contributor to the position for each cell as below in Figure 4. In some cases, a given cell type label may have different regions where different genes make the largest contribution. For example, the Normoblast class has two sub-regions, with the strongest contributors being HBD and HBB. Notably, the sub-region with HBD as the largest gene contributor also extends to the neighboring Erythroblast cluster.\n\nUMAP colored by top features\n#plotly UMAP embedding colored by top gene features\nplot_interactive(\n    reducer, \n    adata_final,\n    groupby=GROUPBY_KEY,\n    color_by='top_gene',\n    fit_index=0,\n    summary_file=summary_interactive_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 4: The PyTorch (v2.9.0) UMAP embedding colored by top gene feature, showing that some cell types have regions with different top gene contributors, and some top gene contributors extend across type divisons. This plot only shows 84% of the points in the dataset, as the top features for the remaining points are more unique and would require a longer legend.\n\n\n\n\n\n\n\n\nTop gene features by cell type\nWe can also generate plots for the average feature contribution for each class in Figure 5, similar to visualizations found in Chari and Pachter (2023). Note that the largest feature contributors do not always point in the direction of the centroid. This gives rise to a variation of contributions for that feature for individual cells across a cluster.\nWith 16 separate UMAP fits at different random initializations, we provide the standard error of the normalized mean contribution of each feature. The feature contributions are normalized by the mean embedding distance of the class for a given fit, since a class could be close to the origin for one fit, and far away from the origin in another fit.\n\nUMAP with top features as vectors for all cell types\ngroup_to_plot = adata_final.obs[GROUPBY_KEY].value_counts().index[:12].tolist()\nplot_feature_importance_by_group(\n    reducer,\n    adata_final,\n    groupby=GROUPBY_KEY,\n    groups_to_plot=group_to_plot,\n    fit_index=0,\n    summary_stats_file=summary_stats_file if not TRAIN else None,\n    summary_plot_file=summary_plot_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Reticulocyte, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(b) Reticulocyte, UMAP feature length, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) CD4+ T naive, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(d) CD4+ T naive, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) CD8+ T naive, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(f) CD8+ T naive, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) CD14+ mono, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(h) CD14+ mono, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i) CD4+ T activated, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(j) CD4+ T activated, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k) Naive CD20+ B IGKC+, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(l) Naive CD20+ B IGKC+, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m) Naive CD20+ B IGKC-, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(n) Naive CD20+ B IGKC-, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(o) Erythroblast, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(p) Erythroblast, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(q) Normoblast, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(r) Normoblast, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(s) NK, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(t) NK, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u) Transitional B, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(v) Transitional B, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(w) CD8+ T CD57+ CD45RA+, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(x) CD8+ T CD57+ CD45RA+, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\nFigure 5: The top gene features for each cell type. Note that the largest feature vectors do not always point to the centroid, often indicating a gradient of importance for that feature across the cluster. Error bars are generated by normalizing the feature importance vectors for each cell by the distance to the centroid of the class for that UMAP fit to account for changing cluster centroids across fits.\n\n\n\n\n\nDot plots\nWe can compare the features found by the Jacobian to differential expression with dot plots for each as in Figure 6.\nWe find that many features identified by differential expression between cell types are not preserved in the Jacobian representation. This highlights how the Jacobian method provides a complementary view of the feature space to features from differential expression.\n\nDot plots\nimport warnings\nsc.settings.verbosity = 0\ncompare_with_differential_expression(\n    reducer,\n    adata_final,\n    groupby=GROUPBY_KEY,\n    n_top_genes=3,\n    summary_stats_file=summary_stats_file if not TRAIN else None,\n    summary_plot_file=summary_plot_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The dot plot for the top differential expression features by cell type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The dot plot for the top Jacobian features by cell type.\n\n\n\n\n\n\n\nFigure 6: Dot plots for gene expression analysis."
  },
  {
    "objectID": "index_v1.0.html#conclusion",
    "href": "index_v1.0.html#conclusion",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Conclusion",
    "text": "Conclusion\nThis work presents a novel approach to interpreting UMAP embeddings by utilizing glass-box deep networks. We have shown how to overcome the black-box nature of nonlinear dimensionality reduction for UMAP by implementing a locally linear (but globally nonlinear) embedding function. This enables the precise quantification of feature attributions for each data point in the UMAP embedding space, directly quantifying the contribution of individual genes to cell positions for gene expression data. This stands in contrast to conventional methods, such as differential expression, which provide only a proxy for what UMAP has learned."
  },
  {
    "objectID": "examples/demo.html",
    "href": "examples/demo.html",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#introduction",
    "href": "examples/demo.html#introduction",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#text",
    "href": "examples/demo.html#text",
    "title": "A brief syntax demo",
    "section": "Text",
    "text": "Text\n\nHeaders\nh1 headers (# &lt;HEADER-TEXT&gt;) are reserved for the title of the pub, so use h2 (## &lt;HEADER-TEXT&gt;) for section titles and h3, h4, etc. for sub-sections.\n\n\nCallouts\nTo draw more attention to a piece of text, use callouts:\n\n\n\n\n\n\nImportant\n\n\n\nThe most effective way to see the rendered pub is to setup a live preview that re-renders the pub whenever you save this file. Do that with make preview.\n\n\n\n\nCitations & Footnotes\nTo cite something, add its bibtex entry to ref.bib and then cite it (Avasthi et al., 2024). Here’s another (Lin et al., 2023). For in-depth description of available citation syntax, visit Quarto’s documentation.\nAlso, don’t forget about footnotes1.\n1 To add additional information, like what you’re reading right now, use footnotes.\nTo create a multi-paragraph footnote, indent subsequent paragraphs. Footnotes can also cite things (Avasthi et al., 2024)."
  },
  {
    "objectID": "examples/demo.html#math",
    "href": "examples/demo.html#math",
    "title": "A brief syntax demo",
    "section": "Math",
    "text": "Math\nRender math2 using standard \\(\\LaTeX\\) syntax. Inline with $...$ and display with $$...$$.\n2 Quarto uses MathJax for math rendering.\\[\ne^{ \\pm i\\theta } = \\cos \\theta \\pm i\\sin \\theta\n\\tag{1}\\]\nEuler’s equation (Equation 1) is pretty."
  },
  {
    "objectID": "examples/demo.html#code",
    "href": "examples/demo.html#code",
    "title": "A brief syntax demo",
    "section": "Code",
    "text": "Code\nWrite code as you would in any Jupyter notebook.\n\ndef alertness(hours_sleep, coffees):\n    base_alertness = min(hours_sleep / 8 * 100, 100)\n    coffee_boost = min(coffees * 30, 60)\n    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\nprint(\"Alertness stats:\")\nprint(f\"4hrs sleep + 1 coffee: {alertness(4, 1)}%\")\nprint(f\"8hrs sleep + 0 coffee: {alertness(8, 0)}%\")\nprint(f\"2hrs sleep + 3 coffee: {alertness(2, 3)}%\")\n\nAlertness stats:\n4hrs sleep + 1 coffee: 70.0%\n8hrs sleep + 0 coffee: 100.0%\n2hrs sleep + 3 coffee: 85.0%\n\n\n\nVisibility & Placement\nSpecify per-block instructions with comments at the top of the code block.\nFold the code block (#| code-fold: true):\n\n\nSource code for this table\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [True, True, False], \"c\": [\"marco\", \"polo\", \"marco\"]})\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\nSuppress code block visibility while retaining the cell output (#| echo: false):\n\n\n\n\n\n\nNote\n\n\n\nThe code block below runs and the output is visible, but the code itself is absent from the rendering.\n\n\n\n\nThe code that generated this print statement is hidden.\n\n\nRender the output in different places, like in the right margin (#| column: margin):\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\nIn general, content placement in highly customizable. For more options, see this Quarto resource.\n\n\nAnnotation\nYou can annotate lines of code. Lines will reveal their annotation when the user hovers over the circled number on the right hand side of the code block.\n\ndef alertness(hours_sleep, coffees):\n1    base_alertness = min(hours_sleep / 8 * 100, 100)\n2    coffee_boost = min(coffees * 20, 60)\n3    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\n1\n\nScale to percentage, cap at 100\n\n2\n\nEach coffee adds 20%, max 60% boost\n\n3\n\nCap total at 100%\n\n\n\n\nFor details, see Quarto’s code annotation documentation.\n\n\nCodebase\nYou can choose to either fold (#| code-fold: true) or supress (#| echo: false) code snippets that distract from the narrative. However, if you’ve written an extensive amount of code, it may be more practical to define it in a package that this notebook imports from, rather than defining it in the notebook itself. This project is already set up to import from packages found in the src/ directory, so place any such code there. As an example, this code block imports code from a placeholder analysis package found at src/analysis.\n\n1from analysis import polo_if_marco\n\npolo_if_marco(\"marco\")\n\n\n1\n\nSource code\n\n\n\n\n'polo'\n\n\nIf you want to package any of your code for the purposes of simplifying this notebook, replace the contents of src/analysis/ with your own package."
  },
  {
    "objectID": "examples/demo.html#figures-tables",
    "href": "examples/demo.html#figures-tables",
    "title": "A brief syntax demo",
    "section": "Figures & Tables",
    "text": "Figures & Tables\n\nCaptions & Labeling\nIn general, if a cell output is a figure or table, you should caption and label it.\n\ndf\n\n\n\nTable 1: This is a small table.\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\n\n\n\nThis is how you reference Table 1.\n\n\n\n\n\n\nNote\n\n\n\nIf the cell output is a table, the label ID should be prefixed with tbl-. If it’s a figure, prefix with fig-.\nFor example, a table could be captioned and labeled with:\n#| label: tbl-small-table\n#| tbl-cap: \"This is a small table.\"\nAnd a figure could be captioned and labeled with:\n#| label: fig-some-figure\n#| fig-cap: \"This is some figure.\"\n\n\nIf your code block produces several plots, you can subcaption each:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef random_plot():\n    plt.figure()\n    plt.scatter(np.random.rand(10), np.random.rand(10), marker=\"o\")\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in range(4):\n    random_plot()\n\n\n\n\n\n\n\n\n\n\n\n(a) This is the first plot.\n\n\n\n\n\n\n\n\n\n\n\n(b) This is the second.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The third.\n\n\n\n\n\n\n\n\n\n\n\n(d) And finally, here’s the fourth.\n\n\n\n\n\n\n\nFigure 1: A panel of scatter plots.\n\n\n\nFigure 1 is just one example layout for multi-panel figures. For more customization options, see Quarto’s documentation on figures.\n\n\nInteractivity\nInteractive widgets can be used. For example, Plotly:\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\npx.scatter(\n    df,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    animation_frame=\"year\",\n    animation_group=\"country\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=55,\n    range_x=[100, 100000],\n    range_y=[25, 90],\n)\n\n                                                \n\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s possible that your local preview fails to render the above widget, and you instead see something to the effect of:\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\nIf you want to see how your widget renders within the pub, run make execute and then start a new preview (make preview)."
  },
  {
    "objectID": "pages/FAQ.html",
    "href": "pages/FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications.\n\n\n\nThere is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget.\n\n\n\nAll the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository.\n\n\n\nReproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started.\n\n\n\nWe welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "pages/FAQ.html#what-is-this",
    "href": "pages/FAQ.html#what-is-this",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-comment",
    "href": "pages/FAQ.html#how-can-i-comment",
    "title": "FAQ",
    "section": "",
    "text": "There is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget."
  },
  {
    "objectID": "pages/FAQ.html#where-is-the-datacode",
    "href": "pages/FAQ.html#where-is-the-datacode",
    "title": "FAQ",
    "section": "",
    "text": "All the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-reproduce-this",
    "href": "pages/FAQ.html#how-can-i-reproduce-this",
    "title": "FAQ",
    "section": "",
    "text": "Reproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-contribute",
    "href": "pages/FAQ.html#how-can-i-contribute",
    "title": "FAQ",
    "section": "",
    "text": "We welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "pages/CONTRIBUTING.html",
    "href": "pages/CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome improvements to this publication! If you’d like to improve or extend the publication, please submit a pull request. We’ll collaborate with you to incorporate your revisions. Alternatively, you’re welcome to leave a comment on the pub using Giscus.\n\nDid you spot any mistakes?\nDo you think an analysis is missing?\nDo you think the wording could be improved?\nDid you spot a typo or grammatical mistake?\n\nThese are just a few examples of revisions that we’d be happy to receive from you.\n\n\n\n\n\n\nNote\n\n\n\nTo learn about how we credit external collaborators, click here.\n\n\n\n\nIf you haven’t already, follow our setup guide to create a local copy of the code and compute environment.\n\n\n\nEdit index.ipynb to your liking.\n\n\n\nTo publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/CONTRIBUTING.html#getting-started",
    "href": "pages/CONTRIBUTING.html#getting-started",
    "title": "Contributing",
    "section": "",
    "text": "If you haven’t already, follow our setup guide to create a local copy of the code and compute environment."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#make-your-changes",
    "href": "pages/CONTRIBUTING.html#make-your-changes",
    "title": "Contributing",
    "section": "",
    "text": "Edit index.ipynb to your liking."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#steps-before-publishing",
    "href": "pages/CONTRIBUTING.html#steps-before-publishing",
    "title": "Contributing",
    "section": "",
    "text": "To publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/SETUP.html",
    "href": "pages/SETUP.html",
    "title": "Setup",
    "section": "",
    "text": "This document details how to create a local copy of this pub’s codebase, setup your compute environment, and reproduce the pub itself. This will enable you to experiment with the analysis in the pub and, optionally, contribute revisions to it.\n\n\nThe codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/glass-box-umap.git --recurse-submodules\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n glass-box-umap --file env.yml\nconda activate glass-box-umap\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#obtain-local-copy",
    "href": "pages/SETUP.html#obtain-local-copy",
    "title": "Setup",
    "section": "",
    "text": "The codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/glass-box-umap.git --recurse-submodules"
  },
  {
    "objectID": "pages/SETUP.html#installation",
    "href": "pages/SETUP.html#installation",
    "title": "Setup",
    "section": "",
    "text": "Important\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n glass-box-umap --file env.yml\nconda activate glass-box-umap\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#reproduce",
    "href": "pages/SETUP.html#reproduce",
    "title": "Setup",
    "section": "Reproduce",
    "text": "Reproduce\nThe best way to ensure you’ve correctly set up your code and compute environment is to reproduce this work. Fortunately, the analysis, and therefore the publication itself, can be reproduced with the following command:\nmake execute\n(Make sure you’re in the conda environment you created above)\nThis will execute and render the notebook index.ipynb, then build the publication site. To preview the site, use\nmake preview\nThis will open a local instance of the publication in your default browser."
  },
  {
    "objectID": "pages/SETUP.html#modify",
    "href": "pages/SETUP.html#modify",
    "title": "Setup",
    "section": "Modify",
    "text": "Modify\nTo modify or extend any analyses, open up index.ipynb with Jupyter or your favorite IDE. To preview changes as you modify the notebook, run make preview again and leave the command running. As you make changes to the notebook, the preview site will automatically reload."
  },
  {
    "objectID": "pages/SETUP.html#publish",
    "href": "pages/SETUP.html#publish",
    "title": "Setup",
    "section": "Publish",
    "text": "Publish\nIf you’ve improved the publication, consider contributing so we can update the hosted publication with your edits (big or small!). To get started, see our contributing guide."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "",
    "text": "UMAP is a ubiquitous tool for low-dimensional visualization of high-dimensional datasets. UMAP learns a low-dimensional mapping from the nearest-neighbor graph structure of a dataset, often producing visually distinct clusters of data that align with known labels (e.g., cell types in a gene expression dataset). While the learned relationship between the input features and the embedding positions can be useful, the nonlinear UMAP embedding function also makes it difficult to directly interpret the mapping in terms of the input features.\nHere, we show how to enable interpretation of the nonlinear mapping through a modification of the parametric UMAP approach, which learns the embedding with a deep network that is locally linear (but still globally nonlinear) with respect to the input features. This allows for the computation of a set of exact feature contributions as linear weights that determine the embedding of each data point. By computing the exact feature contribution for each point in a dataset, we directly quantify which features are most responsible for forming each cluster in the embedding space. We explore the feature contributions for a gene expression dataset from this “glass-box” augmentation of UMAP and compare them with features found by differential expression."
  },
  {
    "objectID": "index.html#purpose",
    "href": "index.html#purpose",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "",
    "text": "UMAP is a ubiquitous tool for low-dimensional visualization of high-dimensional datasets. UMAP learns a low-dimensional mapping from the nearest-neighbor graph structure of a dataset, often producing visually distinct clusters of data that align with known labels (e.g., cell types in a gene expression dataset). While the learned relationship between the input features and the embedding positions can be useful, the nonlinear UMAP embedding function also makes it difficult to directly interpret the mapping in terms of the input features.\nHere, we show how to enable interpretation of the nonlinear mapping through a modification of the parametric UMAP approach, which learns the embedding with a deep network that is locally linear (but still globally nonlinear) with respect to the input features. This allows for the computation of a set of exact feature contributions as linear weights that determine the embedding of each data point. By computing the exact feature contribution for each point in a dataset, we directly quantify which features are most responsible for forming each cluster in the embedding space. We explore the feature contributions for a gene expression dataset from this “glass-box” augmentation of UMAP and compare them with features found by differential expression."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Introduction",
    "text": "Introduction\nUMAP (Uniform Manifold Approximation and Projection) is a powerful tool for nonlinear dimensionality reduction (McInnes et al., 2018). Despite some critical appraisals focused on the use of relative distances over the nonlinear embedding space to generate hypotheses as well as the black-box nature of the nonlinear mapping (Chari and Pachter, 2023; Ng et al., 2023), UMAP remains popular in many fields. Here, we present an augmentation to conventional UMAP analysis that generates exact feature attributions for each point in the dataset.\nPrincipal components analysis (PCA) is another popular method for dimensionality reduction, which finds an alternative linear representation of a dataset by determining orthogonal directions of maximal variance. Since the principal components are the linear weights on input features, this approach is directly interpretable in feature space.\nThe recent popularity of UMAP comes at the cost of interpretable embeddings due to its nonlinearity. While nonlinear methods are generally thought to be black boxes, there are a range of post-hoc feature attribution methods that provide some measure of interpretability (like differential expression applied for gene expression data using ScanPy (v1.11.4) (Wolf et al., 2018) as well as GradCAM (Selvaraju et al., 2017) for image data). UMAP is popular due to its ability to successfully cluster classes for complex datasets in an unsupervised manner, despite its black-box nature.\nUMAP generates distinct clusters as a black box, while PCA provides (sometimes less distinct) clusters for complex datasets, but also provides exact feature contributions. What if we could have the best of both approaches? A technique for interpreting nonlinear deep networks (Mohan et al., 2019; Wang et al., 2016) provides the key for bringing exact feature interpretability to UMAP."
  },
  {
    "objectID": "index.html#method",
    "href": "index.html#method",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Method",
    "text": "Method\nUMAP embeds high-dimensional data into a low-dimensional representation by building a nearest-neighbor graph in the original space and directly learning a set of embeddings in the representation space that best preserves local and global components of the nearest-neighbor graph according to a loss function.\nThe extension to a “parametric” form of UMAP (Pascarelli, 2023; Sainburg et al., 2021), where a deep network learns a function to generate a low-dimensional mapping, is a valuable generalization of the embedding approach, allowing new data points to be quickly embedded using the same mapping function. The deep network is trained using the same loss as the non-parametric model, so the network function captures the same relationships as the original non-parametric implementation.\nThe deep network approach for parametric UMAP is conventionally considered to be opaque to feature attribution. However, by leveraging a growing body of work in this area, we can implement a deep network with a specific architecture that enables us to measure the exact contributions of each input feature.\nIn Wang et al. (2016), Mohan et al. (2019) and Elhage et al. (2021), it is demonstrated that deep networks with zero-bias linear layers and specific types of activation functions possess exactly equivalent linear mappings. Even though these networks are globally nonlinear, which is why deep networks can learn such complex mappings, they are also locally linear or “point-wise” linear for a particular input (Golden, 2025). These networks fall into the category of homogeneous functions of order 1, which, based on Euler’s theorem, means a function has an equivalent representation with the Jacobian \\(J\\) which varies as a function of \\(x\\):\n\\[\ny(x) = J(x) \\cdot x\n\\tag{1}\\]\nThis mapping is linear and exact, although the Jacobian must be numerically computed for each input. Linear representations offer a straightforward approach to understanding what the network is computing. They are more interpretable than locally nonlinear networks (which include any deep network with nonzero bias terms in its linear layers).\nIt is straightforward to compute these linear feature contributions for every point in a dataset with a GPU. These types of deep networks for genomics data are locally linear, and from an interpretability perspective, they are effectively globally linear. We can easily perform an exhaustive local analysis, where we compute the Jacobian (via autograd) for every point in a dataset. With globally linear systems, there is only one set of feature weights to analyze. However, with locally linear systems, there are as many Jacobians (feature weights) as data points, which adds an additional step to the analysis.\nThe exactness of this Jacobian approach is the centerpiece of its appeal. This local analysis is similar to SHAP (Lundberg and Lee, 2017), LIME (Ribeiro et al., 2016), and GradCAM (Selvaraju et al., 2017); however, these methods are approximations that may be incorrect for the actual nonlinear function. The Jacobian of a zero-bias ReLU deep network weights each feature linearly, quantifying how the globally nonlinear network uses those features to generate its output.\nHere, we apply these deep networks with linear equivalents to UMAP. In many papers, UMAP is presented as an interesting visual representation of data, but it is not frequently used beyond that. Conventionally, differential expression is applied to various clusters to identify genes that are differentially expressed on average (which is distinct from the features used by UMAP). In contrast, with fully interpretable glass-box networks, we can compute the exact contribution of each gene to the position of every single cell shown in the UMAP embedding space. Now, the exact gene feature contributions can be directly extracted from the nonlinear UMAP function, rather than from differential expression, which only acts as a proxy for what UMAP has learned. Additionally, the Jacobian approach works equally as well for image or protein embedding representations, where tools like differential expression are not available.\nBeyond the feature attributions for each point, these can be aggregated over categories (like Leiden cluster or cell type) to generate hypotheses about the gene features connected to phenotypes represented in the dataset (York and Mets, 2025). This can be done in a straightforward manner by computing the feature attributions for all points of a given cell type (or Leiden cluster) and measuring summary statistics, like the mean or the singular value decomposition of the feature contributions.\nThe use of glass-box deep networks for UMAP, therefore, provides clarity into what the UMAP embedding function has actually learned."
  },
  {
    "objectID": "index.html#loading-the-data-and-tools",
    "href": "index.html#loading-the-data-and-tools",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Loading the data and tools",
    "text": "Loading the data and tools\nFor an example dataset, we will use the human bone marrow gene expression data of Luecken et al. (2021), which is the example dataset now included in ScanPy (v1.11.4).\n\n\nConfigure training parameters\nimport os\nimport anndata as ad\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import font_manager as fm, pyplot as plt\n\n# Config\nTRAIN = False \n\nN_FITS = 16\nN_FITS_TO_LOAD = 1\nN_PCS = 50\nEPOCHS = 64\nRANDOM_STATE = 42\nGROUPBY_KEY = 'cell_type'\nMODEL_PATH_PATTERN = \"models/umap_{i}.pth\"\nSUMMARY_BASENAME = \"saved_outputs/bmmc_features_rev\"\nBATCH_KEY = \"Samplename\"\n\nsummary_stats_file = f\"{SUMMARY_BASENAME}_stats.csv\"\nsummary_plot_file = f\"{SUMMARY_BASENAME}_plot_data.npz\"\nsummary_interactive_file = f\"{SUMMARY_BASENAME}_interactive.csv\"\n\n\n\n\nData and preprocessing methods\nimport os\nimport subprocess\nimport scanpy as sc\nimport anndata as ad\nimport pandas as pd\n\ndef download_bone_marrow_data(\n    url=\"ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE194nnn/GSE194122/suppl/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad.gz\",\n    filename=\"GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad.gz\"\n) -&gt; ad.AnnData:\n    \"\"\"\n    Downloads, unzips, and loads the bone marrow dataset.\n    \"\"\"\n    unzipped_filename = filename.replace(\".gz\", \"\")\n    if not os.path.isfile(unzipped_filename):\n        if not os.path.isfile(filename):\n            subprocess.run([\"wget\", url, \"--no-verbose\"])\n        subprocess.run([\"gunzip\", filename])\n    \n    return sc.read_h5ad(unzipped_filename)\n\ndef preprocess_adata(\n    adata: ad.AnnData,\n    min_genes: int = 100,\n    min_cells: int = 3,\n    n_top_genes: int = 2000,\n    n_pcs: int = 50,\n    batch_key: str = \"Samplename\",\n    run_scrublet: bool = False\n) -&gt; ad.AnnData:\n    \"\"\"\n    Runs the full scRNA-seq preprocessing pipeline on an AnnData object.\n\n    Args:\n        adata (ad.AnnData): The raw AnnData object.\n        min_genes (int): Min genes for cell filtering.\n        min_cells (int): Min cells for gene filtering.\n        n_top_genes (int): Number of highly variable genes to select.\n        n_pcs (int): Number of principal components to compute.\n        batch_key (str): The key in .obs for batch correction (if any).\n        run_scrublet (bool): Whether to run doublet detection.\n\n    Returns:\n        ad.AnnData: The processed AnnData object.\n    \"\"\"\n    print(\"--- Starting Preprocessing ---\")\n    \n    # 1. Initial setup and QC gene flagging\n    adata.obs_names_make_unique()\n    adata.var_names_make_unique()\n    adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n    adata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n    adata.var[\"hb\"] = adata.var_names.str.contains(\"^HB[^(P)]\")\n    \n    # 2. Calculate QC\n    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, log1p=True)\n    \n    # 3. Remove QC genes\n    genes_to_remove = adata.var[\"mt\"] | adata.var[\"ribo\"] \n    adata._inplace_subset_var(~genes_to_remove)\n    \n    # 4. Filter cells, genes, and detect doublets\n    sc.pp.filter_cells(adata, min_genes=min_genes)\n    sc.pp.filter_genes(adata, min_cells=min_cells)\n    if run_scrublet: \n        sc.pp.scrublet(adata, batch_key=batch_key)\n    \n    # 5. Normalize and find HVGs\n    adata.layers[\"counts\"] = adata.X.copy()\n    sc.pp.normalize_total(adata)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, batch_key=batch_key)\n    \n    # 6. Run PCA\n    sc.tl.pca(adata, n_comps=n_pcs, use_highly_variable=True)\n    \n    return adata\n\n\nThe preprocessing pipeline follows the standard procedure for the dataset in the ScanPy (v1.11.4) clustering tutorial. We take the extra step of filtering out the less common cell types to simplify visualizations and keep only the top 12.\n\n\nLoad and preprocess data\ndef prepare_data(groupby_key: str = 'cell_type', n_pcs: int = 50, batch_key: str = \"Samplename\") -&gt; ad.AnnData:\n    \"\"\"\n    Loads, concatenates, and preprocesses the scRNA-seq data.\n    \"\"\"\n    adata_raw = download_bone_marrow_data()\n    adata_raw.var_names_make_unique()\n\n    # Slice and concatenate data\n    if groupby_key == 'cell_type':\n        adata_subset1 = adata_raw[adata_raw.obs['Samplename'] == 'site1_donor1_cite', :].copy()\n        adata_subset3 = adata_raw[adata_raw.obs['Samplename'] == 'site1_donor3_cite', :].copy()\n        adata_filtered = ad.concat([adata_subset1, adata_subset3], label=\"donors\")\n    else:\n        adata_filtered = adata_raw\n        \n    adata_filtered.obs_names_make_unique()\n\n    # Run the preprocessing pipeline on ALL concatenated cells first\n    adata_processed = preprocess_adata(\n        adata_filtered,  # &lt;-- Use the unfiltered data\n        n_top_genes=2000,\n        n_pcs=n_pcs,\n        batch_key=batch_key\n    )\n\n    # Now, filter to the top cell types for your analysis\n\n    if groupby_key == 'cell_type':\n        top_cell_types = adata_processed.obs[groupby_key].value_counts().nlargest(12).index\n        # This subset now contains the .obsm['X_pca'] that was calculated on ALL cells\n        adata_final = adata_processed[adata_processed.obs[groupby_key].isin(top_cell_types)].copy()\n    if groupby_key.lower() == 'cd4+_t_cell_type':  \n        adata_subset1 = adata_processed[adata_processed.obs['cell_type'] == 'CD4+ T activated', :].copy()\n        adata_subset3 = adata_processed[adata_processed.obs['cell_type'] == 'CD4+ T naive', :].copy()\n\n        adata_t_cells = ad.concat([adata_subset1,adata_subset3], label=\"T_cell_type\")\n\n        adata_final = scp.preprocess_adata(\n            adata_t_cells,\n            n_top_genes=1000, # You can use fewer HVGs for a subset\n            n_pcs=50,         # You need fewer PCs for a subset\n            batch_key=\"Samplename\"\n        )\n\n    return adata_final\n\nadata_final = prepare_data(\n        groupby_key=GROUPBY_KEY, \n        n_pcs=N_PCS, \n        batch_key=BATCH_KEY\n    )\n\n\nWe will also load a set of tools including the “UMAP PyTorch” toolbox in addition to ScanPy (v1.11.4), and define a custom PyTorch (v2.9.0) network to learn an embedding.\nWe perform several independent UMAP fits to the data, starting from different random initializations, to generate error bars for the feature contributions.\n\n\nGlassBoxUMAP class\nimport os\nimport sys\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\n# --- Import PUMAP submodule ---\n# (Assuming 'external/umap_pytorch' is in the working directory)\nSUBMODULE_RELATIVE_PATH = 'external/umap_pytorch' \nproject_root = os.getcwd()\nsubmodule_root = os.path.join(project_root, SUBMODULE_RELATIVE_PATH)\nif submodule_root not in sys.path:\n    sys.path.insert(0, submodule_root) \ntry:\n    from umap_pytorch.main import PUMAP\nexcept ImportError:\n    print(f\"Error: Could not import PUMAP from {submodule_root}.\")\n    print(\"Please ensure the submodule exists and is initialized.\")\n    sys.exit(1)\n# ------------------------------\n\nclass GlassBoxUMAP:\n    \"\"\"\n    Encapsulates the parametric UMAP model fitting and feature attribution.\n    \n    This class follows a scikit-learn style API:\n    1. Initialize with hyperparameters.\n    2. Fit with pre-processed data (e.g., PCA).\n    3. Compute attributions using PCA data, gene expression, and PCA components.\n    \"\"\"\n    def __init__(self,\n                 # PUMAP params\n                 n_components: int = 2,\n                 n_neighbors: int = 15, \n                 min_dist: float = 0.3, \n                 repulsion_strength: float = 3.0,\n                 # Training params\n                 n_fits: int = 1, \n                 epochs: int = 64, \n                 lr: float = 1e-4, \n                 batch_size: int = 2048, \n                 random_state: int = 12,\n                 # Network params\n                 input_size: int = 50,\n                 hidden_size: int = 1024 + 128\n                 ):\n        \"\"\"Initializes the model with all hyperparameters.\"\"\"\n        self.n_components = n_components\n        self.n_neighbors, self.min_dist, self.repulsion_strength = n_neighbors, min_dist, repulsion_strength\n        self.n_fits, self.epochs, self.lr, self.batch_size, self.random_state = n_fits, epochs, lr, batch_size, random_state\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.models_ = []\n        self.embeddings_ = []\n        self.jacobians_ = []\n        self.feature_contributions_ = []\n        self.device_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    def fit(self, X: np.ndarray, \n            load_models: bool = False, \n            load_n_fits: int = 1,\n            save_models: bool = True,\n            model_path_pattern: str = \"models/umap_{i}.pth\"):\n        \"\"\"\n        Fits the Parametric UMAP model to the input data X.\n\n        Args:\n            X (np.ndarray): The input data (e.g., PCA embeddings), \n                            shape (n_samples, n_features).\n            load_models (bool): If True, skips training and loads pre-trained\n                                models from `model_path_pattern`.\n            save_models (bool): If True, saves the trained model weights to\n                                `model_path_pattern` after fitting.\n            model_path_pattern (str): A string pattern for the model file paths.\n        \"\"\"\n        self.train_data_ = torch.tensor(X, dtype=torch.float32)\n        \n        self.models_ = []\n        self.embeddings_ = []\n\n        for i in range(self.n_fits):\n            set_global_seeds(2*self.random_state + i)\n            network = deepReLUNet(\n                input_size=self.input_size, \n                hidden_size=self.hidden_size,\n                output_size=self.n_components\n            )\n            \n            pumap_model = PUMAP(\n                encoder=network, n_neighbors=self.n_neighbors, \n                min_dist=self.min_dist, random_state=self.random_state + i,\n                lr=self.lr, epochs=0 if load_models else self.epochs, # Only 1 epoch if loading\n                batch_size=self.batch_size, num_workers=8, num_gpus=1\n            )\n\n            # Train or load\n            model_file = model_path_pattern.format(i=i)\n            \n            if load_models:\n                if i &lt; load_n_fits:                                \n                    try:\n                        # print(pumap_model.trainer)\n                        # pumap_model.trainer.load_from_checkpoint('/home/ubuntu/james/glass-box-umap/lightning_logs/version_110/checkpoints/epoch=63-step=320.ckpt')\n                        pumap_model.device = self.device_\n                        pumap_model.encoder.to(self.device_)\n                        # We must \"fit\" with 1 epoch to initialize the graph\n                        pumap_model.fit(self.train_data_) \n                        pumap_model.encoder.load_state_dict(\n                            torch.load(model_file, map_location=self.device_)\n                        )\n                        pumap_model.encoder.eval() \n\n                        self.models_.append(pumap_model)\n                        embedding = pumap_model.transform(self.train_data_) \n                        self.embeddings_.append(embedding)\n\n                    except FileNotFoundError:\n                        print(f\"Error: Model file not found at {model_file}\")\n                        raise\n                    except Exception as e:\n                        print(f\"Error loading state dict for model {i}: {e}\")\n                        raise\n                    \n            else:\n                pumap_model.fit(self.train_data_)\n\n                if save_models:\n                    os.makedirs(os.path.dirname(model_file), exist_ok=True)\n                    torch.save(pumap_model.encoder.state_dict(), model_file)\n            \n                self.models_.append(pumap_model)\n                embedding = pumap_model.transform(self.train_data_) \n                self.embeddings_.append(embedding)\n            \n        return self\n\n    def transform(self, X: np.ndarray, fit_index: int = 0) -&gt; np.ndarray:\n        \"\"\"\n        Transforms new data X into the embedding space using a trained model.\n\n        Args:\n            X (np.ndarray): New data to transform.\n            fit_index (int): Index of the model to use for transformation.\n\n        Returns:\n            np.ndarray: The UMAP embedding.\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"The model must be fitted before transforming.\")\n        if fit_index &gt;= len(self.models_):\n            raise IndexError(\"fit_index is out of bounds.\")\n        \n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        return self.models_[fit_index].transform(X_tensor)\n\n    def fit_transform(self, X: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"\n        Fits the model to X and returns the embedding of X.\n        \n        Args:\n            X (np.ndarray): The input data (e.g., PCA embeddings).\n            **kwargs: Additional arguments passed to self.fit().\n\n        Returns:\n            np.ndarray: The UMAP embedding for the first fit (fit_index=0).\n        \"\"\"\n        self.fit(X, **kwargs)\n        return self.embeddings_[0]\n\n    def compute_attributions(self, \n                             X_centered_gene_expression: np.ndarray, \n                             pca_components: np.ndarray, \n                             jacobian_batch_size: int = 40):\n        \"\"\"\n        Computes the Jacobian and projects it to the original gene space.\n\n        Args:\n            X_centered_gene_expression (np.ndarray): Mean-centered gene expression\n                data, shape (n_samples, n_genes).\n            pca_components (np.ndarray): The PCA loading matrix (e.g., adata.varm[\"PCs\"]),\n                shape (n_genes, n_pcs).\n            jacobian_batch_size (int): Batch size for Jacobian calculation.\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"The model must be fitted before computing contributions.\")\n        if self.train_data_ is None:\n             raise RuntimeError(\"self.train_data_ not set. Please call fit() first.\")\n        \n        self.feature_contributions_ = []\n        self.jacobians_ = []\n\n        for i, model in enumerate(self.models_):\n            encoder = model.encoder\n            encoder.eval()\n            \n            # 1. Compute Jacobian in batches (in PCA space)\n            num_samples = self.train_data_.shape[0]\n            jacobians_pca_list = []\n            for j in range(0, num_samples, jacobian_batch_size):\n                data_batch = self.train_data_[j:j + jacobian_batch_size, :]\n                \n                jac_batch = torch.autograd.functional.jacobian(\n                    encoder, data_batch, vectorize=True, strategy=\"reverse-mode\"\n                )\n                # Un-fuse the vectorized diagonal output\n                jac_batch_unfused = torch.einsum('bibj-&gt;bij', jac_batch)\n                jacobians_pca_list.append(jac_batch_unfused.detach().cpu())\n\n            jacobians_pca_tensor = torch.cat(jacobians_pca_list, dim=0)\n\n            # 2. Project Jacobian from PCA space back to gene space\n            # J_gene[i, emb, gene] = J_pca[i, emb, pc] * PCs[gene, pc]\n            gene_space_jacobian = torch.einsum(\n                'bij,kj-&gt;bik', \n                jacobians_pca_tensor, \n                torch.tensor(pca_components, dtype=torch.float32)\n            )\n\n            # 3. Weight by each cell's mean-centered gene expression\n            feature_contributions = gene_space_jacobian.numpy() * X_centered_gene_expression[:, np.newaxis, :]\n            \n            # For memory efficiency\n            feature_contributions = feature_contributions.astype('float16')\n            \n            self.feature_contributions_.append(feature_contributions)\n            self.jacobians_.append(jacobians_pca_tensor)\n\n        return self\n\n    def get_feature_importance(self, \n                               adata: 'ad.AnnData', \n                               groupby: str,\n                               gene_names: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"\n        Aggregates feature contributions by a specified group.\n\n        Args:\n            adata (ad.AnnData): The AnnData object, needed for .obs groupings.\n            groupby (str): The column in adata.obs to group by (e.g., 'cell_type').\n            gene_names (np.ndarray): An array of gene names.\n\n        Returns:\n            pd.DataFrame: A DataFrame with mean and SEM contributions for\n                          each gene in each group.\n        \"\"\"\n        if not self.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first.\")\n\n        all_run_jacobians = np.array(self.feature_contributions_)\n        n_runs = all_run_jacobians.shape[0]\n        all_groups = adata.obs[groupby].cat.categories\n        \n        summary_dfs = []\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            if np.sum(is_group_mask) == 0:\n                continue # Skip if group has no cells\n\n            # Shape: (n_runs, n_cells_in_group, n_dims, n_genes)\n            jacobians_for_group = all_run_jacobians[:, is_group_mask, :, :]\n            \n            # Calculate magnitudes (L2 norm across UMAP dims)\n            # Shape: (n_runs, n_cells_in_group, n_genes)\n            magnitudes = np.linalg.norm(jacobians_for_group, axis=2, ord=2)\n            \n            run_mean_contributions = []\n            for run_idx in range(n_runs):\n                run_mags = magnitudes[run_idx, :, :] # (n_cells, n_genes)\n                \n                # Normalize each cell by its own total contribution\n                cell_sums = np.sum(run_mags, axis=1, keepdims=True)\n                normalized_mags = run_mags / (cell_sums + 1e-9)\n\n                # Get the mean contribution for each gene across cells *for this run*\n                run_mean_contributions.append(np.mean(normalized_mags, axis=0))\n            \n            # Aggregate across all runs\n            # Shape: (n_runs, n_genes)\n            run_means_array = np.array(run_mean_contributions) \n            \n            # Final stats across runs\n            mean_contributions = np.mean(run_means_array, axis=0)\n            sem_contributions = np.std(run_means_array, axis=0) / np.sqrt(n_runs)\n\n            df = pd.DataFrame({\n                'gene': gene_names, \n                'mean_contribution': mean_contributions,\n                'sem_contribution': sem_contributions, \n                groupby: group\n            })\n            summary_dfs.append(df)\n            \n        return pd.concat(summary_dfs, ignore_index=True)\n\n\n    def save_analysis_summary(self, \n                              adata: 'ad.AnnData', \n                              groupby: str,\n                              basename: str = \"analysis_summary\"):\n        \"\"\"\n        Saves all necessary data for offline plotting and analysis.\n        \n        Args:\n            adata (ad.AnnData): The AnnData object, needed for .obs groupings\n                                and gene names.\n            groupby (str): The column in adata.obs to group by (e.g., 'cell_type').\n            basename (str): The prefix for the three output files.\n        \"\"\"\n        if (not self.feature_contributions_ or \n            not self.embeddings_ or \n            not self.jacobians_ or \n            self.train_data_ is None):\n            raise RuntimeError(\"Must run fit() and compute_attributions() first.\")\n\n        # 1. Save population-level statistics\n        stats_df = self.get_feature_importance(adata, groupby, adata.var_names.values)\n        stats_filename = f\"{basename}_stats.csv\"\n        stats_df.to_csv(stats_filename, index=False)\n\n        # 2. Save plot data (NPZ)\n        mean_vector_dict = {}\n        all_groups = adata.obs[groupby].cat.categories\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            mean_vector_dict[group] = np.mean(self.feature_contributions_[0][is_group_mask], axis=0)\n            \n        jacobxall_first_run = self.feature_contributions_[0]\n        jacobian_magnitude = np.linalg.norm(jacobxall_first_run, axis=1)\n\n        jacobian_0 = self.jacobians_[0]\n        pca_data_0 = self.train_data_.squeeze().detach().cpu().numpy()\n        reconstruction_0 = np.einsum('ijk,ik-&gt;ij', jacobian_0.numpy(), pca_data_0)\n\n        plot_data_filename = f\"{basename}_plot_data.npz\"\n        np.savez_compressed(\n            plot_data_filename,\n            embedding=self.embeddings_[0],\n            group_labels=adata.obs[groupby].values,\n            group_by_key=groupby, # Store the key name\n            mean_jacobian_vectors=mean_vector_dict,\n            jacobian_magnitude=jacobian_magnitude,\n            gene_names=adata.var_names.values,\n            jacobian_reconstruction=reconstruction_0  \n        )\n\n        # 3. Save interactive plot data\n        interactive_df = self._prepare_plotly_df(\n            adata, groupby=groupby, fit_index=0, top_n_genes=8\n        )\n        interactive_filename = f\"{basename}_interactive.csv\"\n        interactive_df.to_csv(interactive_filename, index=False)\n\n    def _prepare_plotly_df(self, \n                           adata: 'ad.AnnData', \n                           groupby: str, \n                           fit_index: int = 0, \n                           top_n_genes: int = 8) -&gt; pd.DataFrame:\n        \"\"\"(Private) Prepares a DataFrame for interactive plotting.\"\"\"\n        embedding = self.embeddings_[fit_index]\n        jacobxall = self.feature_contributions_[fit_index]\n        \n        df = pd.DataFrame(embedding, columns=['UMAP 0', 'UMAP 1'])\n        df[groupby] = adata.obs[groupby].values\n\n        # Calculate squared distance and find top contributing genes\n        gene_dist_sq = jacobxall[:, 0, :]**2 + jacobxall[:, 1, :]**2\n        genes = adata.var.index.values\n        \n        top_gene_indices = np.argsort(gene_dist_sq, axis=1)[:, ::-1][:, :top_n_genes]\n        \n        for i in range(top_n_genes):\n            df[f'gene_{i}'] = genes[top_gene_indices[:, i]]\n            \n        return df\n\n\n\n\nPyTorch (v2.9.0) MLP for UMAP\nfrom torch import nn\nclass LayerNormDetached(nn.Module):\n    '''\n    A LayerNorm implementation where the variance calculation is detached from the\n    computation graph during evaluation, potentially stabilizing training.\n    '''\n    def __init__(self, emb_dim: int):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        '''Forward pass for LayerNormDetached.'''\n        mean = x.mean(dim=-1, keepdim=True)\n        # Detach variance calculation during evaluation\n        if not self.training:\n            var = x.clone().detach().var(dim=-1, keepdim=True, unbiased=False)\n        else:\n            var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n        norm_x = (x - mean) / torch.sqrt(var + 1e-12) # Added epsilon for stability\n        return self.scale * norm_x\n\nclass deepReLUNet(nn.Module):\n    \"\"\"\n    A deep neural network using PReLU activation and LayerNormDetached.\n    \"\"\"\n    def __init__(self, input_size: int = 50, hidden_size: int = 256, output_size: int = 2):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(), LayerNormDetached(hidden_size),\n            nn.Linear(hidden_size, hidden_size, bias=False), nn.PReLU(),\n            nn.Linear(hidden_size, output_size, bias=False)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the PReLU network.\"\"\"\n        return self.model(x)\n\nimport random\nimport random\nimport numpy as np\n# You may also need: import pytorch_lightning as pl\n\ndef set_global_seeds(seed: int):\n    \"\"\"Sets global seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    # Optional: For the Pytorch Lightning trainer\n    # pl.seed_everything(seed) \n    \n    # You might also want deterministic algorithms\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n\n\nFit models for UMAP\n# === 2. Model Fitting ===\nprint(\"\\n=== 2. Model Fitting ===\")\nreducer = GlassBoxUMAP(\n    n_fits=N_FITS,\n    epochs=EPOCHS if TRAIN else 0, \n    random_state=RANDOM_STATE,\n    input_size=N_PCS\n)\n\nreducer.fit(\n    adata_final.obsm['X_pca'],\n    load_models=not TRAIN,\n    load_n_fits=N_FITS_TO_LOAD,\n    save_models=TRAIN,\n    model_path_pattern=MODEL_PATH_PATTERN\n)"
  },
  {
    "objectID": "index.html#umap-of-gene-expression-data",
    "href": "index.html#umap-of-gene-expression-data",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "UMAP of gene expression data",
    "text": "UMAP of gene expression data\nHere we show the embeddings from the convetional, non-parametric UMAP from ScanPy (v1.11.4) as well as the PyTorch (v2.9.0) version of parametric UMAP. For visualizations we use the Arcadia Pycolor toolbox (“Arcadia-pycolor,” 2025) (v0.6.5).\n\n\nPlotting methods\n# pumap_plotting\n\nimport scanpy as sc\nimport anndata as ad\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom adjustText import adjust_text\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport arcadia_pycolor as apc\n\nimport logging\nlogging.getLogger(\"scanpy.runtime\").setLevel(logging.ERROR) \n\ndef setup_plotting_themes():\n    \"\"\"Sets up the plotting themes for matplotlib and plotly.\"\"\"\n    apc.mpl.setup()\n    apc.plotly.setup()\n\ndef plot_scanpy_umap(adata: ad.AnnData, \n                     groupby: str = 'cell_type', \n                     n_neighbors: int = 15, \n                     random_state: int = 13):\n    \"\"\"\n    Computes and plots the standard non-parametric UMAP using Scanpy.\n    \n    Args:\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to color by.\n        n_neighbors (int): Number of neighbors for UMAP.\n        random_state (int): Random state for UMAP.\n    \"\"\"\n    \n    if 'neighbors' not in adata.uns:\n        sc.pp.neighbors(adata, n_neighbors=n_neighbors, use_rep='X_pca')\n    \n    # Compute standard UMAP\n    sc.tl.umap(adata, init_pos='random', random_state=random_state)\n\n    # Order categories by frequency\n    category_order = adata.obs[groupby].value_counts().index.tolist()\n    adata.obs[groupby] = adata.obs[groupby].astype('str').astype(\n        pd.CategoricalDtype(categories=category_order, ordered=True)\n    )\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        ax = sc.pl.umap(\n            adata, color=groupby, size=2,\n            palette=list(apc.palettes.primary), show=False\n        )\n        ax.set_xlabel(\"UMAP 0\")\n        ax.set_ylabel(\"UMAP 1\")\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.set_title(\"\")#f\"{groupby} (Standard UMAP)\")\n        plt.show()\n\ndef plot_parametric_umap(reducer: GlassBoxUMAP, \n                         adata: ad.AnnData, \n                         fit_index: int = 0, \n                         groupby: str = 'cell_type'):\n    \"\"\"\n    Plots the UMAP embedding from a specific parametric model fit.\n\n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        fit_index (int): The index of the fitted model to visualize.\n        groupby (str): The .obs column to color by.\n    \"\"\"\n    if not reducer.embeddings_:\n        raise RuntimeError(\"The reducer must be fitted before plotting.\")\n    \n    resolved_index = fit_index if fit_index &gt;= 0 else len(reducer.embeddings_) + fit_index\n\n    # Order categories by frequency\n    category_order = adata.obs[groupby].value_counts().index.tolist()\n    adata.obs[groupby] = adata.obs[groupby].astype('str').astype(\n        pd.CategoricalDtype(categories=category_order, ordered=True)\n    )\n        \n    # Temporarily assign the parametric embedding to the default UMAP slot\n    adata.obsm['X_umap'] = reducer.embeddings_[fit_index]\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        ax = sc.pl.umap(\n            adata, use_raw=False, color=groupby, size=2,\n            palette=list(apc.palettes.primary),\n            show=False\n        )\n        ax.set_xlabel(\"UMAP 0\")\n        ax.set_ylabel(\"UMAP 1\")\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.set_title(\"\")#\n        # ax.set_title(f\"{groupby} (Parametric UMAP Fit {resolved_index})\")\n        plt.show()\n            \ndef validate_jacobian(\n    reducer: 'GlassBoxUMAP', # Use quotes if class is not yet defined\n    fit_index: int = 0, \n    n_samples: int = 100,\n    dtype: torch.dtype = torch.float64\n):\n    \"\"\"\n    Computes and plots the UMAP embedding vs. its reconstruction from the \n    on-the-fly Jacobian, and plots the reconstruction error.\n    \n    This function uses the computation method from your 'plot_error' example.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        fit_index (int): The index of the fitted model to use.\n        n_samples (int): The number of samples to use for the validation \n                         (from the start of the training set).\n        dtype (torch.dtype): The dtype (e.g., torch.float64) for the computation.\n    \"\"\"\n    \n    if (not reducer.models_ or not reducer.train_data_ is not None):\n        raise RuntimeError(\n            \"Must run fit() first to have models and training data available.\"\n        )\n    if fit_index &gt;= len(reducer.models_):\n        raise IndexError(\"fit_index is out of bounds for reducer.models_.\")\n\n    device = reducer.device_\n    if \"cuda\" not in device:\n        n_samples = 8\n    \n    encoder_casted = reducer.models_[fit_index].encoder.to(device=device, dtype=dtype)\n    encoder_casted.eval()\n\n    if n_samples &gt; reducer.train_data_.shape[0]:\n        n_samples = reducer.train_data_.shape[0]\n        \n    pca_data = reducer.train_data_[:n_samples]\n    data_batch_casted = pca_data.to(device=device, dtype=dtype)\n    \n    jac_batch = torch.autograd.functional.jacobian(\n        encoder_casted, data_batch_casted, vectorize=True, strategy=\"reverse-mode\"\n    )\n    \n    reconstruction = torch.einsum(\n        'bibj,bj-&gt;bi', \n        jac_batch, \n        data_batch_casted\n    )\n    \n    embedding = encoder_casted(data_batch_casted)\n    \n    err = reconstruction - embedding\n    \n    embedding_np = embedding.detach().cpu().numpy()\n    reconstruction_np = reconstruction.detach().cpu().numpy()\n    err_np = err.detach().cpu().numpy()\n\n    try:\n        plot_context = mpl.rc_context({\n            \"figure.facecolor\": apc.parchment, \n            \"axes.facecolor\": apc.parchment\n        })\n    except Exception:\n        print(\"Warning: 'apc.parchment' not found. Using default plot style.\")\n        plot_context = mpl.rc_context({}) \n    with plot_context:\n        \n        plt.figure()\n        \n        plt.scatter(embedding_np[:, :].flatten(), reconstruction_np[:, :].flatten(), alpha=0.5, label=\"Reconstruction (flattened)\")\n        \n        global_min = min(embedding_np.min(), reconstruction_np.min())\n        global_max = max(embedding_np.max(), reconstruction_np.max())\n        \n        # Add a small buffer\n        min_val = global_min - 1\n        max_val = global_max + 1\n        plt.plot([min_val, max_val], [min_val, max_val], 'r', linewidth=2, label=\"Identity (exact)\")\n\n        plt.xlabel('UMAP Embedding')#, labelpad=33.8)\n        plt.ylabel('Jacobian Reconstruction')\n        plt.legend()            \n        # plt.axis('square')\n        plt.xlim(min_val, max_val)\n        plt.ylim(min_val, max_val)\n        ax1=plt.gca()\n        # ax1.set_aspect('equal', adjustable='box')\n        ax1.set_box_aspect(1)\n        plt.show()\n\n        plt.figure()\n        plt.hist(err_np.flatten(), bins=40)\n        # current_ax = plt.gca()\n        # current_ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=5, prune='both'))\n        plt.xlabel('Reconstruction Error')# (Reconstruction - Embedding)', labelpad=20)\n        plt.ylabel('Frequency')\n        ax2=plt.gca()\n        # ax2.set_aspect('equal', adjustable='box')\n        ax2.set_box_aspect(1)\n        # plt.title(f'Histogram of Reconstruction Error ({dtype})')\n        plt.show()\n        \ndef plot_interactive(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    color_by: str = 'group', \n    top_n_to_show: int = 16, \n    show_centroids: bool = False, \n    fit_index: int = 0,\n    summary_file: str = \"analysis_summary_interactive.csv\",\n    show_percentage: bool = False\n):\n    \"\"\"\n    Generates an interactive Plotly UMAP embedding.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n        color_by (str): 'group' (to color by `groupby` key) or 'top_gene'.\n        summary_file (str): Path to the .csv file for loading/saving.\n    \"\"\"\n    import os\n    \n    if summary_file and os.path.exists(summary_file):\n        df = pd.read_csv(summary_file)\n\n        if groupby not in df.columns:\n            print(f\"Warning: Column '{groupby}' not found in {summary_file}.\")\n            \n            potential_cols = [col for col in df.columns if \n                              col not in ['UMAP 0', 'UMAP 1'] and \n                              not col.startswith('gene_')]\n            \n            if len(potential_cols) == 1:\n                old_groupby = potential_cols[0]\n                df = df.rename(columns={old_groupby: groupby})\n            else:\n\n                raise KeyError(\n                    f\"Column '{groupby}' not found in {summary_file}. \"\n                    f\"Found potential group columns: {potential_cols}. \"\n                    \"The summary file may be stale. \"\n                    \"Try running with TRAIN=True to regenerate it.\"\n                )\n\n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate data.\")\n        \n        df = reducer._prepare_plotly_df(adata, groupby, fit_index=fit_index)\n        \n        if summary_file:\n            df.to_csv(summary_file, index=False)\n\n    hover_data = ['gene_0', 'gene_1', 'gene_2', groupby]\n    \n    if color_by == 'group':\n        fig = px.scatter(\n            df, x='UMAP 0', y='UMAP 1', color=groupby,\n            # title=f'Bone Marrow Gene Expression: {groupby}',\n            hover_data={k: True for k in hover_data if k != groupby},\n            category_orders={groupby: df[groupby].astype('category').value_counts().index},\n            color_discrete_sequence=(apc.palettes.primary + apc.palettes.secondary)\n        )\n        grouping_col, data_for_centroids = groupby, df\n    \n    elif color_by == 'top_gene':\n        df['gene_0'] = df['gene_0'].astype(str)\n        top_genes = df['gene_0'].value_counts().nlargest(top_n_to_show).index\n        df_filtered = df[df['gene_0'].isin(top_genes)]\n        percent_shown = len(df_filtered) / len(df)\n        # title = f'Bone Marrow: Top Gene Contributors, ({percent_shown:.1%} of cells shown)'\n        fig = px.scatter(\n            df_filtered, x='UMAP 0', y='UMAP 1', color='gene_0',\n            title=\"\", hover_data=hover_data,\n            category_orders={'gene_0': top_genes},\n            color_discrete_sequence=(apc.palettes.secondary + apc.palettes.primary)\n        )\n        grouping_col, data_for_centroids = 'gene_0', df_filtered\n    \n    else:\n        raise ValueError(\"color_by must be 'group' or 'top_gene'\")\n\n    if show_centroids:\n        centroids = data_for_centroids.groupby(grouping_col)[['UMAP 0', 'UMAP 1']].mean()\n        for label, center in centroids.iterrows():\n            fig.add_annotation(\n                x=center['UMAP 0'], y=center['UMAP 1'], text=f\"&lt;b&gt;{label}&lt;/b&gt;\",\n                showarrow=False, font=dict(size=16, color='black'),\n                align='center', bgcolor='rgba(255, 255, 255, 0.5)', borderpad=4\n            )\n            \n    fig.update_traces(marker_size=3)\n    fig.update_layout(\n        autosize=True, \n        yaxis_scaleanchor=\"x\",\n        legend={'itemsizing': 'constant', 'y': 1, 'x': 1.0, 'yanchor': 'top', 'xanchor': 'left'}\n    )\n\n    apc.plotly.style_plot(fig, monospaced_axes=\"all\")\n    fig.show()\n    if color_by == 'top_gene' and show_percentage:\n        print(f'Bone Marrow: Top Gene Contributors, ({percent_shown:.1%} of cells shown)')\n\ndef plot_feature_importance_by_group(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    n_features_bars: int = 12, \n    n_features_vectors: int = 3,\n    fit_index: int = 0, \n    groups_to_plot: list = None,\n    summary_stats_file: str = \"analysis_summary_stats.csv\",\n    summary_plot_file: str = \"analysis_summary_plot_data.npz\",\n    set_axes_equal: bool = False,\n    plot_sum_features: bool = False\n):\n    \"\"\"\n    Analyzes and visualizes feature contributions for each group.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n        groups_to_plot (list): A list of specific group names to plot. \n                               If None, plots the top 12.\n    \"\"\"\n    import os\n    # Add necessary imports that were implicit in the original\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib as mpl\n    from adjustText import adjust_text\n    # Assuming 'apc' is available in the environment (e.g., import anndata_plotting_context as apc)\n    # Assuming 'ad' (anndata) is available\n    \n    can_load_from_file = (\n        summary_stats_file and os.path.exists(summary_stats_file) and\n        summary_plot_file and os.path.exists(summary_plot_file)\n    )\n    \n    if can_load_from_file:\n        summary_df = pd.read_csv(summary_stats_file)\n        with np.load(summary_plot_file, allow_pickle=True) as data:\n            embedding = data['embedding']\n            \n            try:\n                group_labels_array = data['group_labels']\n                loaded_groupby_key = str(data['group_by_key']) if 'group_by_key' in data else 'cell_type' \n            except KeyError:\n                print(\"Warning: 'group_labels' key not found. Trying fallback 'cell_types'...\")\n                try:\n                    group_labels_array = data['cell_types'] \n                    loaded_groupby_key = 'cell_type' \n                except KeyError:\n                    raise KeyError(\n                        \"Could not find 'group_labels' or 'cell_types' in the .npz file. \"\n                        \"The summary file may be stale or corrupt. \"\n                        \"Try running with TRAIN=True to regenerate it.\"\n                    )\n            \n            mean_jacobian_vectors = data['mean_jacobian_vectors'].item()\n            gene_names_original_order = data['gene_names']\n            \n            if loaded_groupby_key != groupby:\n                print(f\"Warning: File was grouped by '{loaded_groupby_key}', \"\n                      f\"but you requested '{groupby}'. Results may be incorrect.\")\n        \n        all_groups = pd.Series(group_labels_array).value_counts().index\n        \n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate data.\")\n        \n        embedding = reducer.embeddings_[fit_index]\n        group_labels_array = adata.obs[groupby].values\n        all_groups = adata.obs[groupby].value_counts().index\n        gene_names_original_order = adata.var_names.values \n        \n        summary_df = reducer.get_feature_importance(adata, groupby, gene_names_original_order)\n\n        mean_jacobian_vectors = {}\n        for group in all_groups:\n            is_group_mask = (adata.obs[groupby] == group).values\n            mean_jacobian_vectors[group] = np.mean(\n                reducer.feature_contributions_[fit_index][is_group_mask], axis=0\n            )\n\n    if groups_to_plot is None:\n        groups_to_plot = all_groups[:12]\n    \n    # This assumes 'apc' is an imported module available in the scope\n    cmap = (apc.palettes.primary + apc.palettes.secondary).to_mpl_cmap()\n    category_colors = [cmap(i / len(all_groups)) for i in range(len(all_groups))]\n    color_map = {name: color for name, color in zip(all_groups, category_colors)}\n    point_colors = np.array([color_map.get(ct, 'gray') for ct in group_labels_array])\n\n    gene_to_original_index = {gene: i for i, gene in enumerate(gene_names_original_order)}\n\n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        for group in groups_to_plot:\n            \n            # --- Plot 1: Scatter plot with vectors ---\n            \n            # Create new figure without subplots\n            fig1 = plt.figure()#figsize=[6, 6])\n            \n            is_group_mask = (group_labels_array == group)\n\n            # Use plt. scatter, xlabel, ylabel, grid, legend\n            plt.scatter(embedding[:, 0], embedding[:, 1], c=point_colors, s=2, alpha=0.1)\n            plt.scatter(embedding[is_group_mask, 0], embedding[is_group_mask, 1], \n                        c=[color_map.get(group, 'gray')], s=14, marker=\"o\", label=group)\n            plt.xlabel(\"UMAP 0\"); plt.ylabel(\"UMAP 1\")\n            plt.grid(False); plt.legend()\n            \n            # Get current axis for commands that require an axis object\n            current_ax = plt.gca()\n            current_ax.spines[['right', 'top']].set_visible(False)\n            if set_axes_equal:\n                current_ax.set_box_aspect(1)\n            \n            group_df = summary_df[summary_df[groupby] == group].sort_values('mean_contribution', ascending=False)\n            \n            if group_df.empty:\n                print(f\"Skipping plot for {group}: no data found in summary.\")\n                plt.close(fig1) # Close the empty figure\n                continue\n\n            top_bar_indices = group_df.index[:n_features_bars]\n            top_vector_indices = group_df.index[:n_features_vectors]\n            \n            vectors_for_group = mean_jacobian_vectors.get(group)\n            if vectors_for_group is None:\n                print(f\"Skipping vectors for {group}: no mean_jacobian_vectors found.\")\n                continue\n            \n            cluster_centroid = np.mean(embedding[is_group_mask], axis=0)\n            \n            top_gene_names = group_df.loc[top_vector_indices, 'gene']\n            top_gene_original_indices = [gene_to_original_index[gene] for gene in top_gene_names if gene in gene_to_original_index]\n\n            if top_gene_original_indices:\n                max_vector_mag = np.max(np.linalg.norm(vectors_for_group[:, top_gene_original_indices], axis=0))\n                scale_factor = (np.linalg.norm(cluster_centroid) / (max_vector_mag + 1e-6)) * 0.8\n            else:\n                scale_factor = 1.0\n\n            texts = []\n            for gene_idx in top_vector_indices:\n                gene_name = group_df.loc[gene_idx, 'gene']\n                gene_original_index = gene_to_original_index.get(gene_name)\n                \n                if gene_original_index is not None:\n                    vec = vectors_for_group[:, gene_original_index] * scale_factor\n                    # Use plt.arrow and plt.text\n                    plt.arrow(0, 0, vec[0], vec[1], width=0.15, color='k', head_width=0.5, zorder=3)\n                    texts.append(plt.text(vec[0], vec[1], gene_name, fontsize=14,\n                                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=apc.parchment, ec=\"none\", alpha=0.8)))\n            \n            if texts:\n                # adjust_text requires an explicit axis\n                adjust_text(texts, ax=current_ax, arrowprops=dict(arrowstyle=\"-\", color='gray', lw=0.5))\n\n            top_genes = group_df.loc[top_bar_indices, 'gene'][::-1]\n            top_means = group_df.loc[top_bar_indices, 'mean_contribution'][::-1]\n            top_sems = group_df.loc[top_bar_indices, 'sem_contribution'][::-1]\n            \n            if plot_sum_features:\n                # Assuming n_features_vectors is at least 20, or you want the top 'n_features_vectors'\n                # Change the index to top 20 features, or use the existing variable\n                top_path_indices = group_df.index[:220]\n\n                # --- Initialize starting point and list for line segments ---\n                current_pos = np.array([0.0, 0.0])\n                path_points = [current_pos.copy()] # Start the path at (0, 0)\n\n                # --- Calculate the sequential path and plot vectors ---\n                for gene_idx in top_path_indices:\n                    gene_name = group_df.loc[gene_idx, 'gene']\n                    gene_original_index = gene_to_original_index.get(gene_name)\n\n                    if gene_original_index is not None:\n                        \n                        vec = vectors_for_group[:, gene_original_index] \n                        \n                        # --- Plot the vector segment ---\n                        # The vector starts at the end of the previous one (current_pos)\n                        # and points to the new position (current_pos + vec)\n                        plt.arrow(current_pos[0], current_pos[1], vec[0], vec[1], \n                                width=0.15, color='r', head_width=0.0, zorder=4, \n                                label='Sequential Path' if len(path_points) == 1 else None)\n                        \n                        # --- Update position and path points ---\n                        current_pos += vec\n                        path_points.append(current_pos.copy())\n                        \n                        # --- Add text label at the end of the vector segment ---\n                        # plt.text(current_pos[0], current_pos[1], gene_name, fontsize=10, color='r',\n                        #         bbox=dict(boxstyle=\"round,pad=0.1\", fc='white', ec=\"none\", alpha=0.6))\n\n                # --- Plot the path as a single line for clarity (optional) ---\n                path_points_array = np.array(path_points)\n                plt.plot(path_points_array[:, 0], path_points_array[:, 1], 'r--', alpha=0.5, zorder=3)\n\n            fig1.tight_layout()\n            plt.show()\n            \n            # --- Plot 2: Bar chart ---\n            \n            # Create a new, separate figure\n            fig2 = plt.figure()#figsize=[6, 6])\n            \n            # Use plt.barh\n            bars = plt.barh(top_genes, top_means, xerr=top_sems, capsize=3, color=color_map.get(group, 'gray'))\n            \n            # Get current axis for bar_label, spines, and box_aspect\n            current_ax = plt.gca()\n            current_ax.bar_label(bars, labels=[f'{g}' for g in top_genes], padding=5)\n            \n            # Use plt.tick_params, xlabel, ylabel\n            plt.tick_params(axis='y', left=False, labelleft=False)\n            plt.xlabel(\"Normalized feature contribution (mean ± SEM)\") \n            plt.ylabel(\"Genes\")\n            \n            current_ax.spines[['right', 'top']].set_visible(False)\n            if set_axes_equal:\n                current_ax.set_box_aspect(1)\n            current_ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=6, prune='both'))\n            fig2.tight_layout()\n            plt.show()\n            \ndef compare_with_differential_expression(\n    reducer: GlassBoxUMAP,\n    adata: ad.AnnData,\n    groupby: str = 'cell_type',\n    n_top_genes: int = 2, \n    summary_stats_file: str = \"analysis_summary_stats.csv\",\n    summary_plot_file: str = \"analysis_summary_plot_data.npz\" \n):\n    \"\"\"\n    Compares Jacobian features with differential expression via dot plots.\n    \n    Args:\n        reducer (GlassBoxUMAP): The fitted model object.\n        adata (ad.AnnData): The processed AnnData object.\n        groupby (str): The .obs column to use for grouping (e.g., 'cell_type').\n    \"\"\"\n    import os\n    from collections import defaultdict\n    layer_to_use = None\n    \n    can_load_from_file = (\n        summary_stats_file and os.path.exists(summary_stats_file) and\n        summary_plot_file and os.path.exists(summary_plot_file)\n    )\n    \n    if can_load_from_file:\n        summary_df = pd.read_csv(summary_stats_file)\n        \n        jacobian_dict = {}\n        for group in adata.obs[groupby].cat.categories:\n            if group in summary_df[groupby].values:\n                top_genes = summary_df[summary_df[groupby] == group].sort_values(\n                    'mean_contribution', ascending=False\n                )['gene'].values[:n_top_genes]\n                jacobian_dict[group] = list(top_genes)\n        \n        with np.load(summary_plot_file, allow_pickle=True) as data:\n            if 'jacobian_magnitude' not in data or 'gene_names' not in data:\n                print(\"Warning: 'jacobian_magnitude' or 'gene_names' not found in summary file.\")\n            else:\n                loaded_magnitude = data['jacobian_magnitude']\n                loaded_genes = data['gene_names'] # Gene names from the NPZ file\n                \n                # Re-align loaded layer with current adata\n                gene_to_npz_index = {gene: i for i, gene in enumerate(loaded_genes)}\n                new_layer = np.zeros(adata.shape, dtype=loaded_magnitude.dtype)\n                \n                genes_found = 0\n                for i, gene in enumerate(adata.var_names):\n                    if gene in gene_to_npz_index:\n                        new_layer[:, i] = loaded_magnitude[:, gene_to_npz_index[gene]]\n                        genes_found += 1\n                \n                adata.layers['jacobian_magnitude'] = new_layer\n                layer_to_use = 'jacobian_magnitude'\n            \n    else:\n        if not reducer.feature_contributions_:\n            raise RuntimeError(\"Must run compute_attributions() first to generate stats.\")\n        \n        stats_df = reducer.get_feature_importance(adata, groupby, adata.var_names.values)\n        \n        jacobian_dict = {}\n        for group in adata.obs[groupby].cat.categories:\n            if group in stats_df[groupby].values:\n                top_genes = stats_df[stats_df[groupby] == group].sort_values(\n                    'mean_contribution', ascending=False\n                )['gene'].values[:n_top_genes]\n                jacobian_dict[group] = list(top_genes)\n        \n        jacobxall_first_run = reducer.feature_contributions_[0]\n        adata.layers['jacobian_magnitude'] = np.linalg.norm(jacobxall_first_run, axis=1)\n        layer_to_use = 'jacobian_magnitude'\n\n    sc.tl.rank_genes_groups(adata, groupby=groupby, method=\"wilcoxon\", n_genes=n_top_genes)\n\n    de_dict = {\n        name: list(adata.uns['rank_genes_groups']['names'][name])\n        for name in adata.uns['rank_genes_groups']['names'].dtype.names\n    }\n\n    combined_genes = defaultdict(list)\n    seen_genes = set()\n    for d in (de_dict, jacobian_dict):\n        for key, value in d.items():\n            for gene in value:\n                if gene not in seen_genes:\n                    combined_genes[key].append(gene)\n                    seen_genes.add(gene)\n    \n    with mpl.rc_context({\"figure.facecolor\": apc.parchment, \"axes.facecolor\": apc.parchment}):\n        # Plot 1: Differential Expression\n        ax1 = sc.pl.rank_genes_groups_dotplot(\n            adata, var_names=combined_genes, groupby=groupby, \n            standard_scale=\"var\", show=False\n        )\n        # plt.title(\"Differential expression (Combined DE + Jacobian Genes)\")\n        # plt.show()\n\n        # Plot 2: Jacobian Feature Importance\n        if layer_to_use:\n            ax2 = sc.pl.rank_genes_groups_dotplot(\n                adata, var_names=combined_genes, groupby=groupby, \n                layer=layer_to_use, standard_scale=\"var\", show=False\n            )\n            # plt.title(\"Jacobian feature importance\")\n            # plt.show()\n        else:\n            print(\"Warning: Could not plot Jacobian dot plot. 'jacobian_magnitude' data was not found.\")\n        plt.show()\n\ndef set_fonts():\n    \"\"\"(Optional) Setup custom fonts.\"\"\"\n    font_files = fm.findSystemFonts('Suisse Int_l/')\n    if len(font_files)&gt;0:\n        for font_file in font_files:\n            fm.fontManager.addfont(font_file)\n\nset_fonts()\nsetup_plotting_themes()\n\n\nSince the PyTorch (v2.9.0) UMAP implementation is slightly different than the conventional UMAP (v0.5.9.post2), we generate the embeddings using both approach and show them below in Figure 1.\n\nConventional and parametric UMAP plots\nadata_final.obsm['X_parametric_umap_0'] = reducer.embeddings_[0]\n\nplot_scanpy_umap(adata_final, groupby=GROUPBY_KEY)\n\nplot_parametric_umap(reducer, adata_final, fit_index=0, groupby=GROUPBY_KEY)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The conventional ScanPy (v1.11.4) UMAP embedding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The PyTorch (v2.9.0) network UMAP embedding.\n\n\n\n\n\n\n\nFigure 1: The conventional ScanPy (v1.11.4) UMAP and the PyTorch (v2.9.0) network UMAP. It is also possible to directly fit the embedding learned by the conventional UMAP algorithm, but here we show a fit with the PyTorch (v2.9.0) method to demonstrate how they find similar embeddings."
  },
  {
    "objectID": "index.html#exact-decomposition-of-features",
    "href": "index.html#exact-decomposition-of-features",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Exact decomposition of features",
    "text": "Exact decomposition of features\nThe Jacobians are computed for each input over the independent fits. This takes a bit of time: about two minutes per fit on a GPU with 16 GB VRAM (on the order of the time spent fitting the model).\n\n\nCompute feature contributions with equivalent linear mapping (ELM) via the Jacobian\nif TRAIN:\n    adata_mean_zero = adata_final.to_df().values - adata_final.to_df().mean(axis=0).values\n    \n    reducer.compute_attributions(\n        X_centered_gene_expression=adata_mean_zero,\n        pca_components=adata_final.varm[\"PCs\"]\n    )\n    print(SUMMARY_BASENAME)\n    reducer.save_analysis_summary(adata_final, groupby=GROUPBY_KEY, basename=SUMMARY_BASENAME)\n\n\nWe can validate that the Jacobian reconstructs the embedding network output below in Figure 2.\n\nValidate the Jacobian reconstruction of the embedding values\nvalidate_jacobian(\n    reducer, \n    n_samples=200\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) UMAP embedding position vs. the Jacobian reconstruction.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram of reconstruction error at float64. The max error is about 3e-14, approaching machine precision.\n\n\n\n\n\n\n\nFigure 2: Jacobian reconstruction. To validate that the Jacobian reconstructs the UMAP encoder network output, we plot the embedding values against their Jacobian reconstructions and see that they fall on the identity line as well as the histogram of the reconstruction error."
  },
  {
    "objectID": "index.html#pytorch-v2.9.0-umap-with-feature-labels",
    "href": "index.html#pytorch-v2.9.0-umap-with-feature-labels",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "PyTorch (v2.9.0) UMAP with Feature Labels",
    "text": "PyTorch (v2.9.0) UMAP with Feature Labels\nWe can visualize the PyTorch (v2.9.0) embedding and add the top gene contributors to the embedding positions as information in the hovertip in Figure 3. The hovertip information provides feature contributions for each point in the dataset.\n\nEmbedding labeled by cell type\n\nUMAP with top features in hovertip\n#plotly UMAP embedding with data tags\n\nplot_interactive(\n    reducer, \n    adata_final,\n    groupby=GROUPBY_KEY,\n    color_by='group', # or 'top_gene'\n    fit_index=0,\n    summary_file=summary_interactive_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 3: The PyTorch (v2.9.0) UMAP embedding colored by cell type with top genes for each cell labeled in the hover tip.\n\n\n\n\n\n\n\n\nEmbedding labeled by top gene contributor\nThe embedding position can also be colored by the top gene contributor to the position for each cell as below in Figure 4. In some cases, a given cell type label may have different regions where different genes make the largest contribution. For example, the Normoblast class has two sub-regions, with the strongest contributors being HBD and HBB. Notably, the sub-region with HBD as the largest gene contributor also extends to the neighboring Erythroblast cluster.\n\nUMAP colored by top features\n#plotly UMAP embedding colored by top gene features\n# | fig-alt:\n# |   - \"Interactive plotly plot of the PyTorch (v2.9.0) UMAP embedding colored by top gene features.\"\nplot_interactive(\n    reducer, \n    adata_final,\n    groupby=GROUPBY_KEY,\n    color_by='top_gene',\n    fit_index=0,\n    summary_file=summary_interactive_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 4: The PyTorch (v2.9.0) UMAP embedding colored by top gene feature, showing that some cell types have regions with different top gene contributors, and some top gene contributors extend across type divisons. This plot only shows 84% of the points in the dataset, as the top features for the remaining points are more unique and would require a longer legend.\n\n\n\n\n\n\n\n\nTop gene features by cell type\nWe can also generate plots for the average feature contribution for each class in Figure 5, similar to visualizations found in Chari and Pachter (2023). Note that the largest feature contributors do not always point in the direction of the centroid. This gives rise to a variation of contributions for that feature for individual cells across a cluster.\nWith 16 separate UMAP fits at different random initializations, we provide the standard error of the normalized mean contribution of each feature. The feature contributions are normalized by the mean embedding distance of the class for a given fit, since a class could be close to the origin for one fit, and far away from the origin in another fit.\n\nUMAP with top features as vectors for all cell types\ngroup_to_plot = adata_final.obs[GROUPBY_KEY].value_counts().index[:12].tolist()\nplot_feature_importance_by_group(\n    reducer,\n    adata_final,\n    groupby=GROUPBY_KEY,\n    groups_to_plot=group_to_plot,\n    fit_index=0,\n    summary_stats_file=summary_stats_file if not TRAIN else None,\n    summary_plot_file=summary_plot_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Reticulocyte, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(b) Reticulocyte, UMAP feature length, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) CD4+ T naive, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(d) CD4+ T naive, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) CD8+ T naive, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(f) CD8+ T naive, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) CD14+ mono, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(h) CD14+ mono, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i) CD4+ T activated, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(j) CD4+ T activated, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k) Naive CD20+ B IGKC+, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(l) Naive CD20+ B IGKC+, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m) Naive CD20+ B IGKC-, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(n) Naive CD20+ B IGKC-, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(o) Erythroblast, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(p) Erythroblast, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(q) Normoblast, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(r) Normoblast, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(s) NK, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(t) NK, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u) Transitional B, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(v) Transitional B, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(w) CD8+ T CD57+ CD45RA+, UMAP feature vectors for fit 0.\n\n\n\n\n\n\n\n\n\n\n\n(x) CD8+ T CD57+ CD45RA+, UMAP feature lengths, mean over 16 fits.\n\n\n\n\n\n\n\nFigure 5: The top gene features for each cell type. Note that the largest feature vectors do not always point to the centroid, often indicating a gradient of importance for that feature across the cluster. Error bars are generated by normalizing the feature importance vectors for each cell by the distance to the centroid of the class for that UMAP fit to account for changing cluster centroids across fits.\n\n\n\n\n\nDot plots\nWe can compare the features found by the Jacobian to differential expression with dot plots for each as in Figure 6.\nWe find that many features identified by differential expression between cell types are not preserved in the Jacobian representation. This highlights how the Jacobian method provides a complementary view of the feature space to features from differential expression.\n\nDot plots\nimport warnings\nsc.settings.verbosity = 0\ncompare_with_differential_expression(\n    reducer,\n    adata_final,\n    groupby=GROUPBY_KEY,\n    n_top_genes=3,\n    summary_stats_file=summary_stats_file if not TRAIN else None,\n    summary_plot_file=summary_plot_file if not TRAIN else None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The dot plot for the top differential expression features by cell type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The dot plot for the top Jacobian features by cell type.\n\n\n\n\n\n\n\nFigure 6: Dot plots for gene expression analysis."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "From black box to glass box: Making UMAP interpretable with exact feature contributions –",
    "section": "Conclusion",
    "text": "Conclusion\nThis work presents a novel approach to interpreting UMAP embeddings by utilizing glass-box deep networks. We have shown how to overcome the black-box nature of nonlinear dimensionality reduction for UMAP by implementing a locally linear (but globally nonlinear) embedding function. This enables the precise quantification of feature attributions for each data point in the UMAP embedding space, directly quantifying the contribution of individual genes to cell positions for gene expression data. This stands in contrast to conventional methods, such as differential expression, which provide only a proxy for what UMAP has learned."
  }
]